{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9362fbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhodz199\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\" \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:2\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310d5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "# https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d5a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 1024\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f70554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration \n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ca1e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>entity</th>\n",
       "      <th>wikidata_reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27012</th>\n",
       "      <td>Q336181</td>\n",
       "      <td>**what program is a variety show?**</td>\n",
       "      <td>**variety show**</td>\n",
       "      <td>**[[Q336181, variety show, entertainment made ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28822</th>\n",
       "      <td>Q1343008</td>\n",
       "      <td>**What genre of program is hakushaku to yōsei?**</td>\n",
       "      <td>**hakushaku to yōsei**</td>\n",
       "      <td>**[[Q1343008, Hakushaku to Yōsei, media franch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33361</th>\n",
       "      <td>Q48892</td>\n",
       "      <td>**what team does drogba play for 2013?**</td>\n",
       "      <td>**drogba**</td>\n",
       "      <td>**[[Q48892, Didier Drogba, Ivorian association...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27625</th>\n",
       "      <td>Q5271417</td>\n",
       "      <td>**what is diane chamberlain's birthplace?**</td>\n",
       "      <td>**diane chamberlain**</td>\n",
       "      <td>**[[Q5271417, Diane Chamberlain, American writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17627</th>\n",
       "      <td>Q8093</td>\n",
       "      <td>**What's a game published by nintendo**</td>\n",
       "      <td>**nintendo**</td>\n",
       "      <td>**[[Q8093, Nintendo, Japanese multinational vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813</th>\n",
       "      <td>Q7696995</td>\n",
       "      <td>**which program is in the tv genre television ...</td>\n",
       "      <td>**television comedy**</td>\n",
       "      <td>**[[Q7696995, television comedy, television ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32511</th>\n",
       "      <td>Q670376</td>\n",
       "      <td>**where did the arizona diamondbacks play?**</td>\n",
       "      <td>**arizona diamondbacks**</td>\n",
       "      <td>**[[Q670376, Arizona Diamondbacks, baseball te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>Q6176201</td>\n",
       "      <td>**Where was jeffrey p. buzen born**</td>\n",
       "      <td>**jeffrey p. buzen**</td>\n",
       "      <td>**[[Q6176201, Jeffrey P. Buzen, American compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>Q2702756</td>\n",
       "      <td>**who is a publisher of the computer game supe...</td>\n",
       "      <td>**super bomberman**</td>\n",
       "      <td>**[[Q2702756, Super Bomberman, 1993 Super NES ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003</th>\n",
       "      <td>Q55</td>\n",
       "      <td>**where are the netherlands on a world map?**</td>\n",
       "      <td>**netherlands**</td>\n",
       "      <td>**[[Q55, Netherlands, country in Northwestern ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34241 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            qid                                           question  \\\n",
       "27012   Q336181                **what program is a variety show?**   \n",
       "28822  Q1343008   **What genre of program is hakushaku to yōsei?**   \n",
       "33361    Q48892           **what team does drogba play for 2013?**   \n",
       "27625  Q5271417        **what is diane chamberlain's birthplace?**   \n",
       "17627     Q8093            **What's a game published by nintendo**   \n",
       "...         ...                                                ...   \n",
       "7813   Q7696995  **which program is in the tv genre television ...   \n",
       "32511   Q670376       **where did the arizona diamondbacks play?**   \n",
       "5192   Q6176201                **Where was jeffrey p. buzen born**   \n",
       "12172  Q2702756  **who is a publisher of the computer game supe...   \n",
       "33003       Q55      **where are the netherlands on a world map?**   \n",
       "\n",
       "                         entity  \\\n",
       "27012          **variety show**   \n",
       "28822    **hakushaku to yōsei**   \n",
       "33361                **drogba**   \n",
       "27625     **diane chamberlain**   \n",
       "17627              **nintendo**   \n",
       "...                         ...   \n",
       "7813      **television comedy**   \n",
       "32511  **arizona diamondbacks**   \n",
       "5192       **jeffrey p. buzen**   \n",
       "12172       **super bomberman**   \n",
       "33003           **netherlands**   \n",
       "\n",
       "                                          wikidata_reply  \n",
       "27012  **[[Q336181, variety show, entertainment made ...  \n",
       "28822  **[[Q1343008, Hakushaku to Yōsei, media franch...  \n",
       "33361  **[[Q48892, Didier Drogba, Ivorian association...  \n",
       "27625  **[[Q5271417, Diane Chamberlain, American writ...  \n",
       "17627  **[[Q8093, Nintendo, Japanese multinational vi...  \n",
       "...                                                  ...  \n",
       "7813   **[[Q7696995, television comedy, television ge...  \n",
       "32511  **[[Q670376, Arizona Diamondbacks, baseball te...  \n",
       "5192   **[[Q6176201, Jeffrey P. Buzen, American compu...  \n",
       "12172  **[[Q2702756, Super Bomberman, 1993 Super NES ...  \n",
       "33003  **[[Q55, Netherlands, country in Northwestern ...  \n",
       "\n",
       "[34241 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv('./2-NEL_Data/2-csv_format_2/training_data.csv')\n",
    "training_data = training_data.sample(frac=1, random_state=1)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22cb7d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**what program is a variety show?**,**variety show**,**[[Q336181, variety show, entertainment made up of a variety of acts], [Q107020026, Quyi, journal], [Q6022366, Theatre of Odeon, Theatre building that opened in 1875, at Pera, Beyoglu, İstanbul.], [Q66323848, Variety Show and Benefit Performance in Vietnam (NAID 102035872), \"item in the National Archives and Records Administration\\'s holdings\"], [Q79312544, Variety show syndrome: making a diagnosis, scientific article published on 01 November 2003]]**'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = list(training_data['question'] + ',' + training_data['entity'] + ',' + training_data['wikidata_reply'])\n",
    "input_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fce9edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q336181'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text = list(training_data['qid'])\n",
    "target_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acef79a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34241\n"
     ]
    }
   ],
   "source": [
    "X_train_tokenized = tokenizer(['nel: ' + sequence for sequence in input_text], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_train_tokenized = tokenizer(target_text, \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(training_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effdb954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5331dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c005c866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>entity</th>\n",
       "      <th>wikidata_reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>Q4931117</td>\n",
       "      <td>**What country is bo lacy from?**</td>\n",
       "      <td>**bo lacy**</td>\n",
       "      <td>**[[Q4931117, Bo Lacy, American football playe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>Q47526</td>\n",
       "      <td>**which position does football player zico play**</td>\n",
       "      <td>**zico**</td>\n",
       "      <td>**[[Q47526, Zico, Brazilian association footba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Q4707240</td>\n",
       "      <td>**what is alan marks's gender?**</td>\n",
       "      <td>**alan marks**</td>\n",
       "      <td>**[[Q4707240, Alan Marks, English artist and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4418</th>\n",
       "      <td>Q9458</td>\n",
       "      <td>**What is one of muhammad's children's names?**</td>\n",
       "      <td>**muhammad**</td>\n",
       "      <td>**[[Q9458, Muhammad, Arabian religious leader ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Q3181381</td>\n",
       "      <td>**where did johnny doyle die**</td>\n",
       "      <td>**johnny doyle**</td>\n",
       "      <td>**[[Q3181381, Johnny Doyle, Scottish footballe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>Q206</td>\n",
       "      <td>**What is the name of an organization that was...</td>\n",
       "      <td>**stephen harper**</td>\n",
       "      <td>**[[Q206, Stephen Harper, Canadian politician]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>Q16554</td>\n",
       "      <td>**what female actress was born in denver, colo...</td>\n",
       "      <td>**denver**</td>\n",
       "      <td>**[[Q16554, Denver, capital city of the state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Q5205415</td>\n",
       "      <td>**what type of music is dj quixotic**</td>\n",
       "      <td>**dj quixotic**</td>\n",
       "      <td>**[[Q5205415, DJ Quixotic, American DJ]]**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>Q84466</td>\n",
       "      <td>**where did otto nückel die**</td>\n",
       "      <td>**otto nückel**</td>\n",
       "      <td>**[[Q84466, Otto Nückel, painter, graphic desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Q73437</td>\n",
       "      <td>**which song did billy idol write**</td>\n",
       "      <td>**billy idol**</td>\n",
       "      <td>**[[Q73437, Billy Idol, English singer], [Q226...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4837 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           qid                                           question  \\\n",
       "2192  Q4931117                  **What country is bo lacy from?**   \n",
       "1030    Q47526  **which position does football player zico play**   \n",
       "447   Q4707240                   **what is alan marks's gender?**   \n",
       "4418     Q9458    **What is one of muhammad's children's names?**   \n",
       "600   Q3181381                     **where did johnny doyle die**   \n",
       "...        ...                                                ...   \n",
       "2895      Q206  **What is the name of an organization that was...   \n",
       "2763    Q16554  **what female actress was born in denver, colo...   \n",
       "905   Q5205415              **what type of music is dj quixotic**   \n",
       "3980    Q84466                      **where did otto nückel die**   \n",
       "235     Q73437                **which song did billy idol write**   \n",
       "\n",
       "                  entity                                     wikidata_reply  \n",
       "2192         **bo lacy**  **[[Q4931117, Bo Lacy, American football playe...  \n",
       "1030            **zico**  **[[Q47526, Zico, Brazilian association footba...  \n",
       "447       **alan marks**  **[[Q4707240, Alan Marks, English artist and i...  \n",
       "4418        **muhammad**  **[[Q9458, Muhammad, Arabian religious leader ...  \n",
       "600     **johnny doyle**  **[[Q3181381, Johnny Doyle, Scottish footballe...  \n",
       "...                  ...                                                ...  \n",
       "2895  **stephen harper**  **[[Q206, Stephen Harper, Canadian politician]...  \n",
       "2763          **denver**  **[[Q16554, Denver, capital city of the state ...  \n",
       "905      **dj quixotic**         **[[Q5205415, DJ Quixotic, American DJ]]**  \n",
       "3980     **otto nückel**  **[[Q84466, Otto Nückel, painter, graphic desi...  \n",
       "235       **billy idol**  **[[Q73437, Billy Idol, English singer], [Q226...  \n",
       "\n",
       "[4837 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = pd.read_csv('./2-NEL_Data/2-csv_format_2/val_data.csv')\n",
    "val_data = val_data.sample(frac=1, random_state=1)\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e28164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**What country is bo lacy from?**,**bo lacy**,**[[Q4931117, Bo Lacy, American football player]]**'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_val = list(val_data['question'] + ',' + val_data['entity'] + ',' + val_data['wikidata_reply'])\n",
    "input_text_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc4d3791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q4931117'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_val = list(val_data['qid'])\n",
    "target_text_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b827092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4837\n"
     ]
    }
   ],
   "source": [
    "X_val_tokenized = tokenizer(['nel: ' + sequence for sequence in input_text_val], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_val_tokenized = tokenizer(target_text_val, \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(val_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea45a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328b489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b25e1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e38f9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(X_val_tokenized, y_val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df2a27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"NEL_model_normal_2\",\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 500, # Evaluation and Save happens every 50 steps\n",
    "    logging_steps = 500,\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    learning_rate = 1e-3,\n",
    "    adam_epsilon = 1e-8,\n",
    "    num_train_epochs = 5,\n",
    "    report_to=\"wandb\",\n",
    "#     metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9cadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12d0463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 34241\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14270\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/HadyElkady/work/Bachelor_thesis/wandb/run-20220702_065152-1gu5mh7j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hodz199/huggingface/runs/1gu5mh7j\" target=\"_blank\">NEL_model_normal_2</a></strong> to <a href=\"https://wandb.ai/hodz199/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='14270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/14270 2:16:40 < 58:22, 1.22 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.662200</td>\n",
       "      <td>0.408311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.428700</td>\n",
       "      <td>0.340432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.300719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.274350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.251899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.253575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.240483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.246655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.226050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.216427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.219884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.227249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.188400</td>\n",
       "      <td>0.209414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.214956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.209343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.214242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.195873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.204155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.205289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>0.199459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-500/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-1000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-1000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-1000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-1500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-1500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-1500/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-2000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-2000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-2000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-2500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-2500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-2500/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-3000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-3000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-3500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-3500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-1000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-4000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-4000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-1500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-4500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-4500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-2000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-5000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-5000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-2500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-5500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-5500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-3000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-6000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-6000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-3500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-6500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-6500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-4000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-7000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-7000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-4500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-7500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-7500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-5000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-8000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-8000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-5500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-8500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-8500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-6000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-9000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-9000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-6500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-9500\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-9500/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-7000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4837\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_2/checkpoint-10000\n",
      "Configuration saved in NEL_model_normal_2/checkpoint-10000/config.json\n",
      "Model weights saved in NEL_model_normal_2/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_2/checkpoint-7500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from NEL_model_normal_2/checkpoint-8500 (score: 0.19587339460849762).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=0.2533380584716797, metrics={'train_runtime': 8206.5496, 'train_samples_per_second': 20.862, 'train_steps_per_second': 1.739, 'total_flos': 5.579551576530432e+16, 'train_loss': 0.2533380584716797, 'epoch': 3.5})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45265a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
