{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6538c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhodz199\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=NER_full_sentence\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1' \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,3\" \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT= NER_full_sentence\n",
    "\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e3ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "# https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c089a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "#     _, _, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ea28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7bea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration \n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab107c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>input_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A top official of export development bank Banc...</td>\n",
       "      <td>*s* Bancomext | ORG *e* , *s* Mexican | MISC *e*</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the early 1980 s , he met Héctor Elizondo w...</td>\n",
       "      <td>*s* Héctor Elizondo | PER *e*</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 316 comes from the number of one of the so...</td>\n",
       "      <td>*s* BBC | ORG *e*</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some 71 % had bought some stock in the past ye...</td>\n",
       "      <td>*s**e*</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hence, NBC might be able to take, say, a 5 % s...</td>\n",
       "      <td>*s* NBC | ORG *e* , *s* MGM/UA | ORG *e*</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152418</th>\n",
       "      <td>The annual Grand National horse race takes pla...</td>\n",
       "      <td>*s* Grand National | MISC *e* , *s* Aintree Ra...</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152419</th>\n",
       "      <td>I think we have to step back take a hard look ...</td>\n",
       "      <td>*s**e*</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152420</th>\n",
       "      <td>Other disease-causing bacteria in this family ...</td>\n",
       "      <td>*s* Enterobacter | MISC *e* , *s* Citrobacter ...</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152421</th>\n",
       "      <td>Cercle Brugge 4 0 3 1 4 5 3</td>\n",
       "      <td>*s* Cercle Brugge | ORG *e*</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152422</th>\n",
       "      <td>or are you just basic-</td>\n",
       "      <td>*s**e*</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152423 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_text  \\\n",
       "0       A top official of export development bank Banc...   \n",
       "1       In the early 1980 s , he met Héctor Elizondo w...   \n",
       "2       The 316 comes from the number of one of the so...   \n",
       "3       Some 71 % had bought some stock in the past ye...   \n",
       "4       Hence, NBC might be able to take, say, a 5 % s...   \n",
       "...                                                   ...   \n",
       "152418  The annual Grand National horse race takes pla...   \n",
       "152419  I think we have to step back take a hard look ...   \n",
       "152420  Other disease-causing bacteria in this family ...   \n",
       "152421                        Cercle Brugge 4 0 3 1 4 5 3   \n",
       "152422                             or are you just basic-   \n",
       "\n",
       "                                              target_text  word_count  \\\n",
       "0        *s* Bancomext | ORG *e* , *s* Mexican | MISC *e*          25   \n",
       "1                           *s* Héctor Elizondo | PER *e*          17   \n",
       "2                                       *s* BBC | ORG *e*          28   \n",
       "3                                                  *s**e*          18   \n",
       "4                *s* NBC | ORG *e* , *s* MGM/UA | ORG *e*          18   \n",
       "...                                                   ...         ...   \n",
       "152418  *s* Grand National | MISC *e* , *s* Aintree Ra...          11   \n",
       "152419                                             *s**e*          14   \n",
       "152420  *s* Enterobacter | MISC *e* , *s* Citrobacter ...          10   \n",
       "152421                        *s* Cercle Brugge | ORG *e*           9   \n",
       "152422                                             *s**e*           5   \n",
       "\n",
       "        NE_count  %_NE_in_sentence  input_length  \n",
       "0              2               0.1           152  \n",
       "1              2               0.1           103  \n",
       "2              1               0.0           136  \n",
       "3              0               0.0            74  \n",
       "4              2               0.1            79  \n",
       "...          ...               ...           ...  \n",
       "152418         4               0.4            72  \n",
       "152419         0               0.0            63  \n",
       "152420         2               0.2            92  \n",
       "152421         2               0.2            27  \n",
       "152422         0               0.0            22  \n",
       "\n",
       "[152423 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_sample = training_data.sample(frac=0.4, random_state=1)\n",
    "\n",
    "training_data = pd.read_csv('./1-NER_Data/1-csv_format/train/training_data_with_pos.csv')\n",
    "# training_data = pd.read_csv('./1-NER_Data/1-csv_format/train/lower_normal_training.csv')\n",
    "\n",
    "training_data = training_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "training_data['input_length'] = training_data['input_text'].apply(lambda x: len(x))\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b292b6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>input_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A top official of export development bank Banc...</td>\n",
       "      <td>*s* Bancomext | ORG *e* , *s* Mexican | MISC *e*</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the early 1980 s , he met Héctor Elizondo w...</td>\n",
       "      <td>*s* Héctor Elizondo | PER *e*</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 316 comes from the number of one of the so...</td>\n",
       "      <td>*s* BBC | ORG *e*</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some 71 % had bought some stock in the past ye...</td>\n",
       "      <td>*s**e*</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hence, NBC might be able to take, say, a 5 % s...</td>\n",
       "      <td>*s* NBC | ORG *e* , *s* MGM/UA | ORG *e*</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152418</th>\n",
       "      <td>The annual Grand National horse race takes pla...</td>\n",
       "      <td>*s* Grand National | MISC *e* , *s* Aintree Ra...</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152419</th>\n",
       "      <td>I think we have to step back take a hard look ...</td>\n",
       "      <td>*s**e*</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152420</th>\n",
       "      <td>Other disease-causing bacteria in this family ...</td>\n",
       "      <td>*s* Enterobacter | MISC *e* , *s* Citrobacter ...</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152421</th>\n",
       "      <td>Cercle Brugge 4 0 3 1 4 5 3</td>\n",
       "      <td>*s* Cercle Brugge | ORG *e*</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152422</th>\n",
       "      <td>or are you just basic-</td>\n",
       "      <td>*s**e*</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152295 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_text  \\\n",
       "0       A top official of export development bank Banc...   \n",
       "1       In the early 1980 s , he met Héctor Elizondo w...   \n",
       "2       The 316 comes from the number of one of the so...   \n",
       "3       Some 71 % had bought some stock in the past ye...   \n",
       "4       Hence, NBC might be able to take, say, a 5 % s...   \n",
       "...                                                   ...   \n",
       "152418  The annual Grand National horse race takes pla...   \n",
       "152419  I think we have to step back take a hard look ...   \n",
       "152420  Other disease-causing bacteria in this family ...   \n",
       "152421                        Cercle Brugge 4 0 3 1 4 5 3   \n",
       "152422                             or are you just basic-   \n",
       "\n",
       "                                              target_text  word_count  \\\n",
       "0        *s* Bancomext | ORG *e* , *s* Mexican | MISC *e*          25   \n",
       "1                           *s* Héctor Elizondo | PER *e*          17   \n",
       "2                                       *s* BBC | ORG *e*          28   \n",
       "3                                                  *s**e*          18   \n",
       "4                *s* NBC | ORG *e* , *s* MGM/UA | ORG *e*          18   \n",
       "...                                                   ...         ...   \n",
       "152418  *s* Grand National | MISC *e* , *s* Aintree Ra...          11   \n",
       "152419                                             *s**e*          14   \n",
       "152420  *s* Enterobacter | MISC *e* , *s* Citrobacter ...          10   \n",
       "152421                        *s* Cercle Brugge | ORG *e*           9   \n",
       "152422                                             *s**e*           5   \n",
       "\n",
       "        NE_count  %_NE_in_sentence  input_length  \n",
       "0              2               0.1           152  \n",
       "1              2               0.1           103  \n",
       "2              1               0.0           136  \n",
       "3              0               0.0            74  \n",
       "4              2               0.1            79  \n",
       "...          ...               ...           ...  \n",
       "152418         4               0.4            72  \n",
       "152419         0               0.0            63  \n",
       "152420         2               0.2            92  \n",
       "152421         2               0.2            27  \n",
       "152422         0               0.0            22  \n",
       "\n",
       "[152295 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data.drop(training_data[training_data['input_length']> 512].index)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62889891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152295\n"
     ]
    }
   ],
   "source": [
    "X_train_tokenized = tokenizer(['ner: ' + sequence for sequence in training_data[\"input_text\"]], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_train_tokenized = tokenizer(list(training_data[\"target_text\"]), \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(training_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b0fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efcce8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>input_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taiwan firms are now the most important source...</td>\n",
       "      <td>*s* Taiwan | MISC *e* , *s* Dongguan | MISC *e*</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Results of English , Scottish and</td>\n",
       "      <td>*s* English | MISC *e* , *s* Scottish | MISC *e*</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>However , because of the intervention of World...</td>\n",
       "      <td>*s* World War II | MISC *e*</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In ensuing seasons , his stock continued to ri...</td>\n",
       "      <td>*s* Finland | LOC *e*</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TREASURY BILLS : Results of the Monday, Octobe...</td>\n",
       "      <td>*s* U.S. | MISC *e* , *s* TREASURY | ORG *e*</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24871</th>\n",
       "      <td>In March 2012 , he was fined for parking illeg...</td>\n",
       "      <td>*s* Olympiacos | ORG *e*</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24872</th>\n",
       "      <td>They want a winner.</td>\n",
       "      <td>*s**e*</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24873</th>\n",
       "      <td>None of them would , so he became another pres...</td>\n",
       "      <td>*s* Nazis | ORG *e*</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24874</th>\n",
       "      <td>The three American jumpers had been easily the...</td>\n",
       "      <td>*s* American | MISC *e*</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24875</th>\n",
       "      <td>CHICAGO 65 66 .496 5</td>\n",
       "      <td>*s* CHICAGO | ORG *e*</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24876 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_text  \\\n",
       "0      Taiwan firms are now the most important source...   \n",
       "1                      Results of English , Scottish and   \n",
       "2      However , because of the intervention of World...   \n",
       "3      In ensuing seasons , his stock continued to ri...   \n",
       "4      TREASURY BILLS : Results of the Monday, Octobe...   \n",
       "...                                                  ...   \n",
       "24871  In March 2012 , he was fined for parking illeg...   \n",
       "24872                                They want a winner.   \n",
       "24873  None of them would , so he became another pres...   \n",
       "24874  The three American jumpers had been easily the...   \n",
       "24875                               CHICAGO 65 66 .496 5   \n",
       "\n",
       "                                            target_text  word_count  NE_count  \\\n",
       "0       *s* Taiwan | MISC *e* , *s* Dongguan | MISC *e*          13         2   \n",
       "1      *s* English | MISC *e* , *s* Scottish | MISC *e*           5         2   \n",
       "2                           *s* World War II | MISC *e*          29         3   \n",
       "3                                 *s* Finland | LOC *e*          19         1   \n",
       "4          *s* U.S. | MISC *e* , *s* TREASURY | ORG *e*          42         2   \n",
       "...                                                 ...         ...       ...   \n",
       "24871                          *s* Olympiacos | ORG *e*          14         1   \n",
       "24872                                            *s**e*           4         0   \n",
       "24873                               *s* Nazis | ORG *e*          21         1   \n",
       "24874                           *s* American | MISC *e*          13         1   \n",
       "24875                             *s* CHICAGO | ORG *e*           5         1   \n",
       "\n",
       "       %_NE_in_sentence  input_length  \n",
       "0                   0.2            81  \n",
       "1                   0.4            33  \n",
       "2                   0.1           183  \n",
       "3                   0.1           105  \n",
       "4                   0.0           210  \n",
       "...                 ...           ...  \n",
       "24871               0.1            80  \n",
       "24872               0.0            19  \n",
       "24873               0.0           149  \n",
       "24874               0.1            80  \n",
       "24875               0.2            20  \n",
       "\n",
       "[24876 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = pd.read_csv('./1-NER_Data/1-csv_format/val/val_data_with_pos.csv')\n",
    "validation_data = validation_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "validation_data['input_length'] = validation_data['input_text'].apply(lambda x: len(x))\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9fb115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>input_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taiwan firms are now the most important source...</td>\n",
       "      <td>*s* Taiwan | MISC *e* , *s* Dongguan | MISC *e*</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Results of English , Scottish and</td>\n",
       "      <td>*s* English | MISC *e* , *s* Scottish | MISC *e*</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>However , because of the intervention of World...</td>\n",
       "      <td>*s* World War II | MISC *e*</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In ensuing seasons , his stock continued to ri...</td>\n",
       "      <td>*s* Finland | LOC *e*</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TREASURY BILLS : Results of the Monday, Octobe...</td>\n",
       "      <td>*s* U.S. | MISC *e* , *s* TREASURY | ORG *e*</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24871</th>\n",
       "      <td>In March 2012 , he was fined for parking illeg...</td>\n",
       "      <td>*s* Olympiacos | ORG *e*</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24872</th>\n",
       "      <td>They want a winner.</td>\n",
       "      <td>*s**e*</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24873</th>\n",
       "      <td>None of them would , so he became another pres...</td>\n",
       "      <td>*s* Nazis | ORG *e*</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24874</th>\n",
       "      <td>The three American jumpers had been easily the...</td>\n",
       "      <td>*s* American | MISC *e*</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24875</th>\n",
       "      <td>CHICAGO 65 66 .496 5</td>\n",
       "      <td>*s* CHICAGO | ORG *e*</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24862 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_text  \\\n",
       "0      Taiwan firms are now the most important source...   \n",
       "1                      Results of English , Scottish and   \n",
       "2      However , because of the intervention of World...   \n",
       "3      In ensuing seasons , his stock continued to ri...   \n",
       "4      TREASURY BILLS : Results of the Monday, Octobe...   \n",
       "...                                                  ...   \n",
       "24871  In March 2012 , he was fined for parking illeg...   \n",
       "24872                                They want a winner.   \n",
       "24873  None of them would , so he became another pres...   \n",
       "24874  The three American jumpers had been easily the...   \n",
       "24875                               CHICAGO 65 66 .496 5   \n",
       "\n",
       "                                            target_text  word_count  NE_count  \\\n",
       "0       *s* Taiwan | MISC *e* , *s* Dongguan | MISC *e*          13         2   \n",
       "1      *s* English | MISC *e* , *s* Scottish | MISC *e*           5         2   \n",
       "2                           *s* World War II | MISC *e*          29         3   \n",
       "3                                 *s* Finland | LOC *e*          19         1   \n",
       "4          *s* U.S. | MISC *e* , *s* TREASURY | ORG *e*          42         2   \n",
       "...                                                 ...         ...       ...   \n",
       "24871                          *s* Olympiacos | ORG *e*          14         1   \n",
       "24872                                            *s**e*           4         0   \n",
       "24873                               *s* Nazis | ORG *e*          21         1   \n",
       "24874                           *s* American | MISC *e*          13         1   \n",
       "24875                             *s* CHICAGO | ORG *e*           5         1   \n",
       "\n",
       "       %_NE_in_sentence  input_length  \n",
       "0                   0.2            81  \n",
       "1                   0.4            33  \n",
       "2                   0.1           183  \n",
       "3                   0.1           105  \n",
       "4                   0.0           210  \n",
       "...                 ...           ...  \n",
       "24871               0.1            80  \n",
       "24872               0.0            19  \n",
       "24873               0.0           149  \n",
       "24874               0.1            80  \n",
       "24875               0.2            20  \n",
       "\n",
       "[24862 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = validation_data.drop(validation_data[validation_data['input_length']> 512].index)\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2695016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24862\n"
     ]
    }
   ],
   "source": [
    "X_val_tokenized = tokenizer(['ner: ' + sequence for sequence in validation_data[\"input_text\"]], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_val_tokenized = tokenizer(list(validation_data[\"target_text\"]), \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(validation_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea2852ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fa24188",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(X_val_tokenized, y_val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f697fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     \"NER_lower\",\n",
    "#     evaluation_strategy ='steps',\n",
    "#     eval_steps = 500, # Evaluation and Save happens every 500 steps\n",
    "#     save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "#     per_device_train_batch_size = 8,\n",
    "#     per_device_eval_batch_size = 8,\n",
    "#     learning_rate = 1e-3,\n",
    "#     adam_epsilon = 1e-8,\n",
    "#     num_train_epochs = 6,\n",
    "#     report_to=\"wandb\",\n",
    "# #     metric_for_best_model = 'f1',\n",
    "#     load_best_model_at_end=True\n",
    "# )\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"NER_normal_with_pos_v3\",\n",
    "#     evaluation_strategy ='epoch',\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 2380, # Evaluation and Save happens every 3743 steps (steps/(num of epochs*2))\n",
    "#     logging_steps = 500,\n",
    "    save_steps = 2380,\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    \n",
    "    gradient_accumulation_steps =2,\n",
    "    \n",
    "    \n",
    "    learning_rate = 1e-3,\n",
    "    adam_epsilon = 1e-8,\n",
    "    num_train_epochs = 10,\n",
    "    report_to=\"wandb\",\n",
    "#     metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e339627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "#     compute_metrics=compute_metrics,\n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed11bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 152295\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 47590\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/HadyElkady/work/Bachelor_thesis/wandb/run-20220828_165122-1l6nhwos</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hodz199/NER_full_sentence/runs/1l6nhwos\" target=\"_blank\">NER_normal_with_pos_v3</a></strong> to <a href=\"https://wandb.ai/hodz199/NER_full_sentence\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19292' max='47590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19292/47590 10:32:28 < 15:27:49, 0.51 it/s, Epoch 4.05/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.003862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.003991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7140</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.003431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9520</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.003293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.003166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14280</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16660</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.002946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19040</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.002783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 24862\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to NER_normal_with_pos_v3/checkpoint-2380\n",
      "Configuration saved in NER_normal_with_pos_v3/checkpoint-2380/config.json\n",
      "Model weights saved in NER_normal_with_pos_v3/checkpoint-2380/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24862\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to NER_normal_with_pos_v3/checkpoint-4760\n",
      "Configuration saved in NER_normal_with_pos_v3/checkpoint-4760/config.json\n",
      "Model weights saved in NER_normal_with_pos_v3/checkpoint-4760/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24862\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to NER_normal_with_pos_v3/checkpoint-7140\n",
      "Configuration saved in NER_normal_with_pos_v3/checkpoint-7140/config.json\n",
      "Model weights saved in NER_normal_with_pos_v3/checkpoint-7140/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24862\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to NER_normal_with_pos_v3/checkpoint-9520\n",
      "Configuration saved in NER_normal_with_pos_v3/checkpoint-9520/config.json\n",
      "Model weights saved in NER_normal_with_pos_v3/checkpoint-9520/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24862\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to NER_normal_with_pos_v3/checkpoint-11900\n",
      "Configuration saved in NER_normal_with_pos_v3/checkpoint-11900/config.json\n",
      "Model weights saved in NER_normal_with_pos_v3/checkpoint-11900/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24862\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to NER_normal_with_pos_v3/checkpoint-14280\n",
      "Configuration saved in NER_normal_with_pos_v3/checkpoint-14280/config.json\n",
      "Model weights saved in NER_normal_with_pos_v3/checkpoint-14280/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_normal_with_pos_v3/checkpoint-2380] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24862\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to NER_normal_with_pos_v3/checkpoint-16660\n",
      "Configuration saved in NER_normal_with_pos_v3/checkpoint-16660/config.json\n",
      "Model weights saved in NER_normal_with_pos_v3/checkpoint-16660/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_normal_with_pos_v3/checkpoint-4760] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24862\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to NER_normal_with_pos_v3/checkpoint-19040\n",
      "Configuration saved in NER_normal_with_pos_v3/checkpoint-19040/config.json\n",
      "Model weights saved in NER_normal_with_pos_v3/checkpoint-19040/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_normal_with_pos_v3/checkpoint-7140] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6faac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab1d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num examples = 317864\n",
    "# Num Epochs = 3\n",
    "# Instantaneous batch size per device = 8\n",
    "# Total train batch size (w. parallel, distributed & accumulation) = 32\n",
    "# Gradient Accumulation steps = 1\n",
    "# Total optimization steps = 29802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab253220",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ner normal ###\n",
    "# number of steps -> 29802\n",
    "# epochs -> 6\n",
    "# patience -> 5\n",
    "# eval_steps = 1000\n",
    "# save_steps = 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
