{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0534b36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhodz199\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=Relation_Lining\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\" \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import requests\n",
    "import time\n",
    "\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT= Relation_Lining\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734220c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation_label(rel_id):\n",
    "    label = ''\n",
    "    API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'format': 'json',\n",
    "        'languages': 'en',\n",
    "        'props': 'labels',\n",
    "        'ids': ''\n",
    "    }\n",
    "      \n",
    "    params['ids'] = str(rel_id)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_ENDPOINT, params = params).json()['entities']\n",
    "        label = response[str(rel_id)]['labels']['en']['value']\n",
    "    except:\n",
    "        return label\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c698c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import inspect\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def parallal_task(func, iterable, *params): \n",
    "    with open(f'./tmp_func.py', 'w') as file:\n",
    "        file.write(\"import requests \\n\")\n",
    "        file.write(inspect.getsource(func).replace(func.__name__, 'task'))\n",
    "\n",
    "    from tmp_func import task\n",
    "    pool = Pool(processes=15)\n",
    "    res = pool.map(task, iterable)\n",
    "    pool.close()\n",
    "    \n",
    "    os.remove('./tmp_func.py')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2f0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "# https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161affc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 4048\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394b6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration \n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74dff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2aab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31090 entries, 0 to 31226\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   question               31090 non-null  object\n",
      " 1   entity label           31090 non-null  object\n",
      " 2   relation labels        31090 non-null  object\n",
      " 3   target relation id     31090 non-null  object\n",
      " 4   target relation label  31090 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv('./3-Relation_Linking_Data/1-csv_format/training_data.csv')\n",
    "training_data = training_data.dropna()\n",
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af6286d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# training_data['target relation label'] = parallal_task(get_relation_label, list(training_data['target relation id']))\n",
    "# print(time.time() - start)\n",
    "# training_data.to_csv('./3-Relation_Linking_Data/1-csv_format/training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8051fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['input_text'] = '**' + training_data['question'] + '**,**' + training_data['relation labels'] + '**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb7a98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(training_data['input_text'].str.len(), bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d022e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.842071405596656"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data['input_text'][training_data['input_text'].str.len() > 1800])*100/len(training_data['input_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f829f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9865551624316501"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[training_data['input_text'].str.len() <= 4048])/len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fbcfda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>entity label</th>\n",
       "      <th>relation labels</th>\n",
       "      <th>target relation id</th>\n",
       "      <th>target relation label</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>whats the genre for the book titled shadows in...</td>\n",
       "      <td>shadows in flight</td>\n",
       "      <td>[ instance of ,  author ,  publisher ,  part o...</td>\n",
       "      <td>P136</td>\n",
       "      <td>genre</td>\n",
       "      <td>**whats the genre for the book titled shadows ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8921</th>\n",
       "      <td>what is a gameplay feature on gp-1: part ii</td>\n",
       "      <td>gp-1: part ii</td>\n",
       "      <td>[ instance of ,  genre ,  game mode ,  publish...</td>\n",
       "      <td>P404</td>\n",
       "      <td>game mode</td>\n",
       "      <td>**what is a gameplay feature on gp-1: part ii*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15084</th>\n",
       "      <td>what language is spoken on the tv show coal</td>\n",
       "      <td>coal</td>\n",
       "      <td>[ instance of ,  country of origin ,  producti...</td>\n",
       "      <td>P364</td>\n",
       "      <td>original language of film or TV show</td>\n",
       "      <td>**what language is spoken on the tv show coal*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6269</th>\n",
       "      <td>what kind of european is pedro sempson</td>\n",
       "      <td>pedro sempson</td>\n",
       "      <td>[ date of birth ,  instance of ,  given name ,...</td>\n",
       "      <td>P27</td>\n",
       "      <td>country of citizenship</td>\n",
       "      <td>**what kind of european is pedro sempson**,**[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13526</th>\n",
       "      <td>what is the genre of the album this is our music</td>\n",
       "      <td>this is our music</td>\n",
       "      <td>[ performer ,  record label ,  instance of ,  ...</td>\n",
       "      <td>P136</td>\n",
       "      <td>genre</td>\n",
       "      <td>**what is the genre of the album this is our m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846</th>\n",
       "      <td>what types of game is jade empire</td>\n",
       "      <td>jade empire</td>\n",
       "      <td>[ instance of ,  game mode ,  platform ,  publ...</td>\n",
       "      <td>P136</td>\n",
       "      <td>genre</td>\n",
       "      <td>**what types of game is jade empire**,**[ inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18668</th>\n",
       "      <td>What is Efim Geller's gender?</td>\n",
       "      <td>Efim Geller</td>\n",
       "      <td>[ sex or gender ,  VIAF ID ,  ISNI ,  Commons ...</td>\n",
       "      <td>P21</td>\n",
       "      <td>sex or gender</td>\n",
       "      <td>**What is Efim Geller's gender?**,**[ sex or g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td>Where did hughie thomasson die</td>\n",
       "      <td>hughie thomasson</td>\n",
       "      <td>[ sex or gender ,  occupation ,  MusicBrainz a...</td>\n",
       "      <td>P20</td>\n",
       "      <td>place of death</td>\n",
       "      <td>**Where did hughie thomasson die**,**[ sex or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13152</th>\n",
       "      <td>what is the sequel to sleepaway camp</td>\n",
       "      <td>sleepaway camp</td>\n",
       "      <td>[ IMDb ID ,  instance of ,  director ,  cast m...</td>\n",
       "      <td>P156</td>\n",
       "      <td>followed by</td>\n",
       "      <td>**what is the sequel to sleepaway camp**,**[ I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>who is bernard fanning signed with</td>\n",
       "      <td>bernard fanning</td>\n",
       "      <td>[ Commons category ,  Library of Congress auth...</td>\n",
       "      <td>P264</td>\n",
       "      <td>record label</td>\n",
       "      <td>**who is bernard fanning signed with**,**[ Com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28944 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question       entity label  \\\n",
       "273    whats the genre for the book titled shadows in...  shadows in flight   \n",
       "8921         what is a gameplay feature on gp-1: part ii      gp-1: part ii   \n",
       "15084        what language is spoken on the tv show coal               coal   \n",
       "6269              what kind of european is pedro sempson      pedro sempson   \n",
       "13526   what is the genre of the album this is our music  this is our music   \n",
       "...                                                  ...                ...   \n",
       "11846                  what types of game is jade empire        jade empire   \n",
       "18668                      What is Efim Geller's gender?        Efim Geller   \n",
       "5625                      Where did hughie thomasson die   hughie thomasson   \n",
       "13152               what is the sequel to sleepaway camp     sleepaway camp   \n",
       "264                   who is bernard fanning signed with    bernard fanning   \n",
       "\n",
       "                                         relation labels target relation id  \\\n",
       "273    [ instance of ,  author ,  publisher ,  part o...               P136   \n",
       "8921   [ instance of ,  genre ,  game mode ,  publish...               P404   \n",
       "15084  [ instance of ,  country of origin ,  producti...               P364   \n",
       "6269   [ date of birth ,  instance of ,  given name ,...                P27   \n",
       "13526  [ performer ,  record label ,  instance of ,  ...               P136   \n",
       "...                                                  ...                ...   \n",
       "11846  [ instance of ,  game mode ,  platform ,  publ...               P136   \n",
       "18668  [ sex or gender ,  VIAF ID ,  ISNI ,  Commons ...                P21   \n",
       "5625   [ sex or gender ,  occupation ,  MusicBrainz a...                P20   \n",
       "13152  [ IMDb ID ,  instance of ,  director ,  cast m...               P156   \n",
       "264    [ Commons category ,  Library of Congress auth...               P264   \n",
       "\n",
       "                      target relation label  \\\n",
       "273                                   genre   \n",
       "8921                              game mode   \n",
       "15084  original language of film or TV show   \n",
       "6269                 country of citizenship   \n",
       "13526                                 genre   \n",
       "...                                     ...   \n",
       "11846                                 genre   \n",
       "18668                         sex or gender   \n",
       "5625                         place of death   \n",
       "13152                           followed by   \n",
       "264                            record label   \n",
       "\n",
       "                                              input_text  \n",
       "273    **whats the genre for the book titled shadows ...  \n",
       "8921   **what is a gameplay feature on gp-1: part ii*...  \n",
       "15084  **what language is spoken on the tv show coal*...  \n",
       "6269   **what kind of european is pedro sempson**,**[...  \n",
       "13526  **what is the genre of the album this is our m...  \n",
       "...                                                  ...  \n",
       "11846  **what types of game is jade empire**,**[ inst...  \n",
       "18668  **What is Efim Geller's gender?**,**[ sex or g...  \n",
       "5625   **Where did hughie thomasson die**,**[ sex or ...  \n",
       "13152  **what is the sequel to sleepaway camp**,**[ I...  \n",
       "264    **who is bernard fanning signed with**,**[ Com...  \n",
       "\n",
       "[28944 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data[training_data['input_text'].str.len() <= 2024]\n",
    "training_data = training_data.sample(frac=1, random_state=1)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7a68094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**whats the genre for the book titled shadows in flight**,**[ instance of ,  author ,  publisher ,  part of the series ,  OCLC control number ,  ISBN-13 ,  genre ,  characters ,  Freebase ID ,  country of origin ,  publication date ,  language of work or name ,  ISFDB title ID ,  NooSFere book ID ,  title ,  OCLC work ID ,  place of publication ,  takes place in fictional universe ,  Goodreads version/edition ID ,  Goodreads work ID ,  cover art by ,  form of creative work ,  follows ,  FantLab work ID ,  set in environment ,  Open Library ID ]**'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = list(training_data['input_text'])\n",
    "input_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18f1b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genre'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text = list(training_data['target relation label'].astype(str))\n",
    "target_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d620f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28944\n"
     ]
    }
   ],
   "source": [
    "X_train_tokenized = tokenizer(['relation_linking: ' + sequence for sequence in input_text], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_train_tokenized = tokenizer(target_text, \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(training_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "280ef7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4423\n"
     ]
    }
   ],
   "source": [
    "validation_data = pd.read_csv('./3-Relation_Linking_Data/1-csv_format/validation_data.csv')\n",
    "validation_data\n",
    "print(len(validation_data))\n",
    "validation_data = validation_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ced4b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# validation_data['target relation label'] = parallal_task(get_relation_label, list(validation_data['target relation id']))\n",
    "# print(time.time() - start)\n",
    "# validation_data.to_csv('./3-Relation_Linking_Data/1-csv_format/validation_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d628c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['input_text'] = '**' + validation_data['question'] + '**,**' + validation_data['relation labels'] + '**'\n",
    "validation_data = validation_data[validation_data['input_text'].str.len() <= 1700]\n",
    "validation_data = validation_data.sample(frac=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0edf72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_val = validation_data['input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dabf71a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genre'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_val = list(validation_data['target relation label'].astype(str))\n",
    "target_text_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44905ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3976\n"
     ]
    }
   ],
   "source": [
    "X_val_tokenized = tokenizer(['relation_linking: ' + sequence for sequence in input_text_val], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_val_tokenized = tokenizer(target_text_val, \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(validation_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10af96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d873b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(X_val_tokenized, y_val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d41d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"Relation_linking_without_entity_higher_patience\",\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 1000, # Evaluation and Save happens every 50 steps\n",
    "    logging_steps = 1000,\n",
    "    save_steps = 1000,\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    learning_rate = 1e-3,\n",
    "    adam_epsilon = 1e-8,\n",
    "    num_train_epochs = 5,\n",
    "    report_to=\"wandb\",\n",
    "#     metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0846da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "#     callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c74769c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 28944\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36180\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/HadyElkady/work/Bachelor_thesis/wandb/run-20220815_164958-3c21vr0l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hodz199/Relation_Lining/runs/3c21vr0l\" target=\"_blank\">Relation_linking_without_entity_higher_patience</a></strong> to <a href=\"https://wandb.ai/hodz199/Relation_Lining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22000' max='36180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22000/36180 4:54:02 < 3:09:32, 1.25 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.071863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.031202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.025268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.030116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.024925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.018685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.029649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.021034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.016720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.015890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.013240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.015088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.014055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.011771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.013516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.012355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.009075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.014677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.012257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.012951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.011285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.011204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-1000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-1000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-1000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-2000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-2000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-2000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-3000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-3000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-3000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-4000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-4000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-4000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-5000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-5000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-5000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-6000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-6000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-1000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-7000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-7000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-2000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-8000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-8000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-3000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-9000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-9000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-4000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-10000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-10000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-5000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-11000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-11000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-6000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-12000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-12000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-7000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-13000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-13000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-8000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-14000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-14000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-9000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-15000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-15000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-10000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-16000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-16000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-11000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-17000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-17000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-12000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-18000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-18000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-13000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-19000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-19000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-14000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-20000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-20000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-15000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-21000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-21000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-16000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3976\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to Relation_linking_without_entity_higher_patience/checkpoint-22000\n",
      "Configuration saved in Relation_linking_without_entity_higher_patience/checkpoint-22000/config.json\n",
      "Model weights saved in Relation_linking_without_entity_higher_patience/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [Relation_linking_without_entity_higher_patience/checkpoint-18000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from Relation_linking_without_entity_higher_patience/checkpoint-17000 (score: 0.00907452404499054).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22000, training_loss=0.02936964464187622, metrics={'train_runtime': 17649.8724, 'train_samples_per_second': 8.199, 'train_steps_per_second': 2.05, 'total_flos': 7.211393519616e+16, 'train_loss': 0.02936964464187622, 'epoch': 3.04})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90fcb33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
