{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908314b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhodz199\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\" \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc6114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "# https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de86f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a9799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration \n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c5ea9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>input_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in february 2013 , beyoncé said that madonna i...</td>\n",
       "      <td>*beyoncé*,*madonna*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The album made its debut at # 8 on the \" Billb...</td>\n",
       "      <td>*Billboard*,*Jewel*,*United States*</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he was a politician , one of the first french ...</td>\n",
       "      <td>*french*,*british*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>andrea collinelli ( italy ) 4:16.141 beat fran...</td>\n",
       "      <td>*andrea collinelli*,*italy*,*francis moreau*,*...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>concerned experts from the reform and developm...</td>\n",
       "      <td>*hainan-rrb-*,*hainan*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318105</th>\n",
       "      <td>He was fired halfway through the 2005 season a...</td>\n",
       "      <td>*Jerry Narron*</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318106</th>\n",
       "      <td>Palmeiras 5 2 3 0 8 1 9</td>\n",
       "      <td>*Palmeiras*</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318107</th>\n",
       "      <td>In 1991 the IFAB made an addition which deemed...</td>\n",
       "      <td>*IFAB*</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318108</th>\n",
       "      <td>he attended the fenway park 100th anniversary ...</td>\n",
       "      <td>*fenway park*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318109</th>\n",
       "      <td>But it showed strength throughout the session,...</td>\n",
       "      <td>*only 2102.2*,*the first few minutes*</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318110 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_text  \\\n",
       "0       in february 2013 , beyoncé said that madonna i...   \n",
       "1       The album made its debut at # 8 on the \" Billb...   \n",
       "2       he was a politician , one of the first french ...   \n",
       "3       andrea collinelli ( italy ) 4:16.141 beat fran...   \n",
       "4       concerned experts from the reform and developm...   \n",
       "...                                                   ...   \n",
       "318105  He was fired halfway through the 2005 season a...   \n",
       "318106                            Palmeiras 5 2 3 0 8 1 9   \n",
       "318107  In 1991 the IFAB made an addition which deemed...   \n",
       "318108  he attended the fenway park 100th anniversary ...   \n",
       "318109  But it showed strength throughout the session,...   \n",
       "\n",
       "                                              target_text  word_count  \\\n",
       "0                                     *beyoncé*,*madonna*         NaN   \n",
       "1                     *Billboard*,*Jewel*,*United States*        31.0   \n",
       "2                                      *french*,*british*         NaN   \n",
       "3       *andrea collinelli*,*italy*,*francis moreau*,*...         NaN   \n",
       "4                                  *hainan-rrb-*,*hainan*         NaN   \n",
       "...                                                   ...         ...   \n",
       "318105                                     *Jerry Narron*        13.0   \n",
       "318106                                        *Palmeiras*         8.0   \n",
       "318107                                             *IFAB*        31.0   \n",
       "318108                                      *fenway park*         NaN   \n",
       "318109              *only 2102.2*,*the first few minutes*        20.0   \n",
       "\n",
       "        NE_count  %_NE_in_sentence  input_length  \n",
       "0            NaN               NaN            93  \n",
       "1            4.0               0.1           165  \n",
       "2            NaN               NaN           100  \n",
       "3            NaN               NaN            76  \n",
       "4            NaN               NaN           205  \n",
       "...          ...               ...           ...  \n",
       "318105       2.0               0.2            75  \n",
       "318106       1.0               0.1            23  \n",
       "318107       1.0               0.0           184  \n",
       "318108       NaN               NaN            78  \n",
       "318109       2.0               0.1           117  \n",
       "\n",
       "[318110 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_sample = training_data.sample(frac=0.4, random_state=1)\n",
    "\n",
    "# training_data = pd.read_csv('./1-NER_Data/1-csv_format/train/training_data.csv')\n",
    "training_data = pd.read_csv('./1-NER_Data/1-csv_format/train/lower_normal_training.csv')\n",
    "\n",
    "training_data = training_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "training_data['input_length'] = training_data['input_text'].apply(lambda x: len(x))\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3fe92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>input_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in february 2013 , beyoncé said that madonna i...</td>\n",
       "      <td>*beyoncé*,*madonna*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The album made its debut at # 8 on the \" Billb...</td>\n",
       "      <td>*Billboard*,*Jewel*,*United States*</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he was a politician , one of the first french ...</td>\n",
       "      <td>*french*,*british*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>andrea collinelli ( italy ) 4:16.141 beat fran...</td>\n",
       "      <td>*andrea collinelli*,*italy*,*francis moreau*,*...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>concerned experts from the reform and developm...</td>\n",
       "      <td>*hainan-rrb-*,*hainan*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318105</th>\n",
       "      <td>He was fired halfway through the 2005 season a...</td>\n",
       "      <td>*Jerry Narron*</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318106</th>\n",
       "      <td>Palmeiras 5 2 3 0 8 1 9</td>\n",
       "      <td>*Palmeiras*</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318107</th>\n",
       "      <td>In 1991 the IFAB made an addition which deemed...</td>\n",
       "      <td>*IFAB*</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318108</th>\n",
       "      <td>he attended the fenway park 100th anniversary ...</td>\n",
       "      <td>*fenway park*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318109</th>\n",
       "      <td>But it showed strength throughout the session,...</td>\n",
       "      <td>*only 2102.2*,*the first few minutes*</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317864 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_text  \\\n",
       "0       in february 2013 , beyoncé said that madonna i...   \n",
       "1       The album made its debut at # 8 on the \" Billb...   \n",
       "2       he was a politician , one of the first french ...   \n",
       "3       andrea collinelli ( italy ) 4:16.141 beat fran...   \n",
       "4       concerned experts from the reform and developm...   \n",
       "...                                                   ...   \n",
       "318105  He was fired halfway through the 2005 season a...   \n",
       "318106                            Palmeiras 5 2 3 0 8 1 9   \n",
       "318107  In 1991 the IFAB made an addition which deemed...   \n",
       "318108  he attended the fenway park 100th anniversary ...   \n",
       "318109  But it showed strength throughout the session,...   \n",
       "\n",
       "                                              target_text  word_count  \\\n",
       "0                                     *beyoncé*,*madonna*         NaN   \n",
       "1                     *Billboard*,*Jewel*,*United States*        31.0   \n",
       "2                                      *french*,*british*         NaN   \n",
       "3       *andrea collinelli*,*italy*,*francis moreau*,*...         NaN   \n",
       "4                                  *hainan-rrb-*,*hainan*         NaN   \n",
       "...                                                   ...         ...   \n",
       "318105                                     *Jerry Narron*        13.0   \n",
       "318106                                        *Palmeiras*         8.0   \n",
       "318107                                             *IFAB*        31.0   \n",
       "318108                                      *fenway park*         NaN   \n",
       "318109              *only 2102.2*,*the first few minutes*        20.0   \n",
       "\n",
       "        NE_count  %_NE_in_sentence  input_length  \n",
       "0            NaN               NaN            93  \n",
       "1            4.0               0.1           165  \n",
       "2            NaN               NaN           100  \n",
       "3            NaN               NaN            76  \n",
       "4            NaN               NaN           205  \n",
       "...          ...               ...           ...  \n",
       "318105       2.0               0.2            75  \n",
       "318106       1.0               0.1            23  \n",
       "318107       1.0               0.0           184  \n",
       "318108       NaN               NaN            78  \n",
       "318109       2.0               0.1           117  \n",
       "\n",
       "[317864 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data.drop(training_data[training_data['input_length']> 512].index)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9991f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317864\n"
     ]
    }
   ],
   "source": [
    "X_train_tokenized = tokenizer(['ner: ' + sequence for sequence in training_data[\"input_text\"]], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_train_tokenized = tokenizer(list(training_data[\"target_text\"]), \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(training_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0647f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7753535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>input_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The average yield on six-month CDs of $ 50,000...</td>\n",
       "      <td>*six-month*,*the week ended Tuesday*,*New York...</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pilate said, `` So you are a king. ''</td>\n",
       "      <td>**</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. Rosen said the quake will revive consumer ...</td>\n",
       "      <td>*1972*,*Rosen*</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To recover from the trauma of killing another ...</td>\n",
       "      <td>*Amazon*</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Add Women 's singles , third round Lisa Raymon...</td>\n",
       "      <td>*Lisa Raymond*,*U.S.*,*Kimberly Po*,*U.S.*</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26546</th>\n",
       "      <td>Previously , WJZ-TV carried the team from thei...</td>\n",
       "      <td>*WJZ-TV*,*Baltimore*</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26547</th>\n",
       "      <td>According to presentations, the quality of Chi...</td>\n",
       "      <td>*China*</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26548</th>\n",
       "      <td>Hindenburg refused the powers but agreed to th...</td>\n",
       "      <td>*Hindenburg*</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26549</th>\n",
       "      <td>He finished fourth in the Olympics that year .</td>\n",
       "      <td>*Olympics*</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26550</th>\n",
       "      <td>CHICAGO 65 66 .496 5</td>\n",
       "      <td>*CHICAGO*</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26551 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_text  \\\n",
       "0      The average yield on six-month CDs of $ 50,000...   \n",
       "1                  Pilate said, `` So you are a king. ''   \n",
       "2      Mr. Rosen said the quake will revive consumer ...   \n",
       "3      To recover from the trauma of killing another ...   \n",
       "4      Add Women 's singles , third round Lisa Raymon...   \n",
       "...                                                  ...   \n",
       "26546  Previously , WJZ-TV carried the team from thei...   \n",
       "26547  According to presentations, the quality of Chi...   \n",
       "26548  Hindenburg refused the powers but agreed to th...   \n",
       "26549     He finished fourth in the Olympics that year .   \n",
       "26550                               CHICAGO 65 66 .496 5   \n",
       "\n",
       "                                             target_text  word_count  \\\n",
       "0      *six-month*,*the week ended Tuesday*,*New York...          32   \n",
       "1                                                     **           9   \n",
       "2                                         *1972*,*Rosen*          32   \n",
       "3                                               *Amazon*          19   \n",
       "4             *Lisa Raymond*,*U.S.*,*Kimberly Po*,*U.S.*          15   \n",
       "...                                                  ...         ...   \n",
       "26546                               *WJZ-TV*,*Baltimore*          14   \n",
       "26547                                            *China*          17   \n",
       "26548                                       *Hindenburg*           9   \n",
       "26549                                         *Olympics*           8   \n",
       "26550                                          *CHICAGO*           5   \n",
       "\n",
       "       NE_count  %_NE_in_sentence  input_length  \n",
       "0             5               0.2           177  \n",
       "1             0               0.0            37  \n",
       "2             2               0.1           189  \n",
       "3             1               0.1           111  \n",
       "4             6               0.4            90  \n",
       "...         ...               ...           ...  \n",
       "26546         2               0.1            91  \n",
       "26547         1               0.1           111  \n",
       "26548         1               0.1            58  \n",
       "26549         1               0.1            46  \n",
       "26550         1               0.2            20  \n",
       "\n",
       "[26551 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = pd.read_csv('./1-NER_Data/1-csv_format/val/val_data.csv')\n",
    "validation_data = validation_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "validation_data['input_length'] = validation_data['input_text'].apply(lambda x: len(x))\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "565523c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>input_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The average yield on six-month CDs of $ 50,000...</td>\n",
       "      <td>*six-month*,*the week ended Tuesday*,*New York...</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pilate said, `` So you are a king. ''</td>\n",
       "      <td>**</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. Rosen said the quake will revive consumer ...</td>\n",
       "      <td>*1972*,*Rosen*</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To recover from the trauma of killing another ...</td>\n",
       "      <td>*Amazon*</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Add Women 's singles , third round Lisa Raymon...</td>\n",
       "      <td>*Lisa Raymond*,*U.S.*,*Kimberly Po*,*U.S.*</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26546</th>\n",
       "      <td>Previously , WJZ-TV carried the team from thei...</td>\n",
       "      <td>*WJZ-TV*,*Baltimore*</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26547</th>\n",
       "      <td>According to presentations, the quality of Chi...</td>\n",
       "      <td>*China*</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26548</th>\n",
       "      <td>Hindenburg refused the powers but agreed to th...</td>\n",
       "      <td>*Hindenburg*</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26549</th>\n",
       "      <td>He finished fourth in the Olympics that year .</td>\n",
       "      <td>*Olympics*</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26550</th>\n",
       "      <td>CHICAGO 65 66 .496 5</td>\n",
       "      <td>*CHICAGO*</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26540 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_text  \\\n",
       "0      The average yield on six-month CDs of $ 50,000...   \n",
       "1                  Pilate said, `` So you are a king. ''   \n",
       "2      Mr. Rosen said the quake will revive consumer ...   \n",
       "3      To recover from the trauma of killing another ...   \n",
       "4      Add Women 's singles , third round Lisa Raymon...   \n",
       "...                                                  ...   \n",
       "26546  Previously , WJZ-TV carried the team from thei...   \n",
       "26547  According to presentations, the quality of Chi...   \n",
       "26548  Hindenburg refused the powers but agreed to th...   \n",
       "26549     He finished fourth in the Olympics that year .   \n",
       "26550                               CHICAGO 65 66 .496 5   \n",
       "\n",
       "                                             target_text  word_count  \\\n",
       "0      *six-month*,*the week ended Tuesday*,*New York...          32   \n",
       "1                                                     **           9   \n",
       "2                                         *1972*,*Rosen*          32   \n",
       "3                                               *Amazon*          19   \n",
       "4             *Lisa Raymond*,*U.S.*,*Kimberly Po*,*U.S.*          15   \n",
       "...                                                  ...         ...   \n",
       "26546                               *WJZ-TV*,*Baltimore*          14   \n",
       "26547                                            *China*          17   \n",
       "26548                                       *Hindenburg*           9   \n",
       "26549                                         *Olympics*           8   \n",
       "26550                                          *CHICAGO*           5   \n",
       "\n",
       "       NE_count  %_NE_in_sentence  input_length  \n",
       "0             5               0.2           177  \n",
       "1             0               0.0            37  \n",
       "2             2               0.1           189  \n",
       "3             1               0.1           111  \n",
       "4             6               0.4            90  \n",
       "...         ...               ...           ...  \n",
       "26546         2               0.1            91  \n",
       "26547         1               0.1           111  \n",
       "26548         1               0.1            58  \n",
       "26549         1               0.1            46  \n",
       "26550         1               0.2            20  \n",
       "\n",
       "[26540 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = validation_data.drop(validation_data[validation_data['input_length']> 512].index)\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c240eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26540\n"
     ]
    }
   ],
   "source": [
    "X_val_tokenized = tokenizer(['ner: ' + sequence for sequence in validation_data[\"input_text\"]], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_val_tokenized = tokenizer(list(validation_data[\"target_text\"]), \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(validation_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d33c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680a67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(X_val_tokenized, y_val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61d05b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     \"NER_lower\",\n",
    "#     evaluation_strategy ='steps',\n",
    "#     eval_steps = 500, # Evaluation and Save happens every 500 steps\n",
    "#     save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "#     per_device_train_batch_size = 8,\n",
    "#     per_device_eval_batch_size = 8,\n",
    "#     learning_rate = 1e-3,\n",
    "#     adam_epsilon = 1e-8,\n",
    "#     num_train_epochs = 6,\n",
    "#     report_to=\"wandb\",\n",
    "# #     metric_for_best_model = 'f1',\n",
    "#     load_best_model_at_end=True\n",
    "# )\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"NER_lower_normal_2\",\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 1000, # Evaluation and Save happens every 500 steps\n",
    "    save_steps = 1000,\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    learning_rate = 1e-3,\n",
    "    adam_epsilon = 1e-8,\n",
    "    num_train_epochs = 3,\n",
    "    report_to=\"wandb\",\n",
    "#     metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ece0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf272dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 317864\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 29802\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/HadyElkady/work/Bachelor_thesis/wandb/run-20220811_111745-t686ha65</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hodz199/huggingface/runs/t686ha65\" target=\"_blank\">NER_lower_normal_2</a></strong> to <a href=\"https://wandb.ai/hodz199/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29802' max='29802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29802/29802 13:55:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.019054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.016653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.014110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.012790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.011849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.010672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.010231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.009883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.009490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.008934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.008411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.008051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.008271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.007711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.007383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.007049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.007030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.006497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.006249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.006501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.006244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.006106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.006092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.005867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.005618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.005648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.005592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.005451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.005353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-1000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-1000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-1000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-2000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-2000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-2000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-3000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-3000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-3000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-4000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-4000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-4000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-5000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-5000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-5000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-6000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-6000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-1000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-7000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-7000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-2000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-8000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-8000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-3000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-9000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-9000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-4000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-10000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-10000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-5000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-11000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-11000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-6000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-12000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-12000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-7000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-13000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-13000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-8000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-14000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-14000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-9000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-15000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-15000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-10000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-16000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-16000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-11000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-17000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-17000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-12000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-18000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-18000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-13000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-19000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-19000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-14000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-20000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-20000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-15000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-21000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-21000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-16000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-22000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-22000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-17000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-23000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-23000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-18000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-24000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-24000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-19000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-25000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-25000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-20000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-26000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-26000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-26000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-21000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-27000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-27000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-22000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-28000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-28000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-23000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26540\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to NER_lower_normal_2/checkpoint-29000\n",
      "Configuration saved in NER_lower_normal_2/checkpoint-29000/config.json\n",
      "Model weights saved in NER_lower_normal_2/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [NER_lower_normal_2/checkpoint-24000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from NER_lower_normal_2/checkpoint-29000 (score: 0.00535267498344183).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=29802, training_loss=0.011344199708288736, metrics={'train_runtime': 50123.3263, 'train_samples_per_second': 19.025, 'train_steps_per_second': 0.595, 'total_flos': 2.5745762425761792e+17, 'train_loss': 0.011344199708288736, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04b42fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7200c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num examples = 317864\n",
    "# Num Epochs = 3\n",
    "# Instantaneous batch size per device = 8\n",
    "# Total train batch size (w. parallel, distributed & accumulation) = 32\n",
    "# Gradient Accumulation steps = 1\n",
    "# Total optimization steps = 29802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6e1ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ner normal ###\n",
    "# number of steps -> 29802\n",
    "# epochs -> 6\n",
    "# patience -> 5\n",
    "# eval_steps = 1000\n",
    "# save_steps = 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
