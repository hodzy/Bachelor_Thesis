{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a897641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.12.17-py2.py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from wandb) (1.16.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp39-cp39-win_amd64.whl (10 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from wandb) (5.8.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from wandb) (61.2.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Collecting promise<3,>=2.0\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from wandb) (3.19.4)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=f6ca6715c9124b9cc712e5e84ee602eeb44540a8d57caa5469e81b43d70a4a37\n",
      "  Stored in directory: c:\\users\\hadye\\appdata\\local\\pip\\cache\\wheels\\e1\\e8\\83\\ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=178b217146dedc312b1ccc9eca9d8dd7a5ca6f644dad3cc5ebf6f27a39d23b77\n",
      "  Stored in directory: c:\\users\\hadye\\appdata\\local\\pip\\cache\\wheels\\b7\\0a\\67\\ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, promise, pathtools, GitPython, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.17\n",
      "Requirement already satisfied: transformers in c:\\users\\hadye\\anaconda3\\lib\\site-packages (4.17.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: click in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\hadye\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "# wandb.login()\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5e36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "# https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b9f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a37735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration \n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45090db0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>*EU*,*German*,*British*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>*Peter Blackburn*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRUSSELS 1996-08-22</td>\n",
       "      <td>*BRUSSELS*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The European Commission said on Thursday it di...</td>\n",
       "      <td>*European Commission*,*German*,*British*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany 's representative to the European Unio...</td>\n",
       "      <td>*Germany*,*European Union*,*Werner Zwingmann*,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159090</th>\n",
       "      <td>\" @Mackenzie_71 : Super Bowl Sunday \"</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159091</th>\n",
       "      <td>Watch Barrow - Boston United Live December 20 ...</td>\n",
       "      <td>*Barrow*,*Boston United*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159092</th>\n",
       "      <td>On January 22nd , from 11- 1.30 , TVPS will be...</td>\n",
       "      <td>*TVPS*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159093</th>\n",
       "      <td>Conditioning plan all made up for tomorrow , f...</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159094</th>\n",
       "      <td>Sat Jan 10 2015 12:46 : 39 GMT+0000 ( UTC )</td>\n",
       "      <td>**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159095 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_text  \\\n",
       "0        EU rejects German call to boycott British lamb .   \n",
       "1                                         Peter Blackburn   \n",
       "2                                     BRUSSELS 1996-08-22   \n",
       "3       The European Commission said on Thursday it di...   \n",
       "4       Germany 's representative to the European Unio...   \n",
       "...                                                   ...   \n",
       "159090              \" @Mackenzie_71 : Super Bowl Sunday \"   \n",
       "159091  Watch Barrow - Boston United Live December 20 ...   \n",
       "159092  On January 22nd , from 11- 1.30 , TVPS will be...   \n",
       "159093  Conditioning plan all made up for tomorrow , f...   \n",
       "159094        Sat Jan 10 2015 12:46 : 39 GMT+0000 ( UTC )   \n",
       "\n",
       "                                              target_text  \n",
       "0                                 *EU*,*German*,*British*  \n",
       "1                                       *Peter Blackburn*  \n",
       "2                                              *BRUSSELS*  \n",
       "3                *European Commission*,*German*,*British*  \n",
       "4       *Germany*,*European Union*,*Werner Zwingmann*,...  \n",
       "...                                                   ...  \n",
       "159090                                                 **  \n",
       "159091                           *Barrow*,*Boston United*  \n",
       "159092                                             *TVPS*  \n",
       "159093                                                 **  \n",
       "159094                                                 **  \n",
       "\n",
       "[159095 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_sample = training_data.sample(frac=0.4, random_state=1)\n",
    "\n",
    "training_data = pd.read_csv('./1-NER_Data/1-csv_format/train/training_data.csv')\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca96801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159095\n"
     ]
    }
   ],
   "source": [
    "X_train_tokenized = tokenizer(['ner: ' + sequence for sequence in training_data[\"input_text\"]], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_train_tokenized = tokenizer(list(training_data[\"target_text\"]), \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(training_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e84a9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"test_trainer\",\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    learning_rate = 1e-3,\n",
    "    adam_epsilon = 1e-8,\n",
    "    num_train_epochs = 5,\n",
    "    report_to=\"wandb\",\n",
    "    save_total_limit = 3 # Only last 3 models are saved. Older ones are deleted.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset= train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da76b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a92d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
