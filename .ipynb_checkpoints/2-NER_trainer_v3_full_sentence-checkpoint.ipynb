{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342053f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhodz199\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=NER_full_sentence\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1' \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\" \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT= NER_full_sentence\n",
    "\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a38927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "# https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1755e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "#     _, _, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9810cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04762a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration \n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "424c17ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>target_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He made his Millwall debut on 2 May 1998 again...</td>\n",
       "      <td>He made his &lt;Millwall&gt; debut on 2 May 1998 aga...</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was revised to suit adult preferences and w...</td>\n",
       "      <td>It was revised to suit adult preferences and w...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The efforts to get to the people who are addic...</td>\n",
       "      <td>The efforts to get to the people who are addic...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the wild , their range has contracted due t...</td>\n",
       "      <td>In the wild , their range has contracted due t...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Second day</td>\n",
       "      <td>Second day</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152498</th>\n",
       "      <td>The annual Grand National horse race takes pla...</td>\n",
       "      <td>The annual &lt;Gran National&gt; horse race takes pl...</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152499</th>\n",
       "      <td>His servants said to him, `` We heard that the...</td>\n",
       "      <td>His servants said to him, `` We heard that the...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152500</th>\n",
       "      <td>Other disease-causing bacteria in this family ...</td>\n",
       "      <td>Other disease-causing bacteria in this family ...</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152501</th>\n",
       "      <td>Cercle Brugge 4 0 3 1 4 5 3</td>\n",
       "      <td>&lt;Cercl Brugge&gt; 4 0 3 1 4 5 3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152502</th>\n",
       "      <td>He took the subway.</td>\n",
       "      <td>He took the subway.</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152503 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_text  \\\n",
       "0       He made his Millwall debut on 2 May 1998 again...   \n",
       "1       It was revised to suit adult preferences and w...   \n",
       "2       The efforts to get to the people who are addic...   \n",
       "3       In the wild , their range has contracted due t...   \n",
       "4                                              Second day   \n",
       "...                                                   ...   \n",
       "152498  The annual Grand National horse race takes pla...   \n",
       "152499  His servants said to him, `` We heard that the...   \n",
       "152500  Other disease-causing bacteria in this family ...   \n",
       "152501                        Cercle Brugge 4 0 3 1 4 5 3   \n",
       "152502                                He took the subway.   \n",
       "\n",
       "                                              target_text  word_count  \\\n",
       "0       He made his <Millwall> debut on 2 May 1998 aga...          19   \n",
       "1       It was revised to suit adult preferences and w...          22   \n",
       "2       The efforts to get to the people who are addic...          60   \n",
       "3       In the wild , their range has contracted due t...          20   \n",
       "4                                              Second day           2   \n",
       "...                                                   ...         ...   \n",
       "152498  The annual <Gran National> horse race takes pl...          11   \n",
       "152499  His servants said to him, `` We heard that the...          15   \n",
       "152500  Other disease-causing bacteria in this family ...          10   \n",
       "152501                       <Cercl Brugge> 4 0 3 1 4 5 3           9   \n",
       "152502                                He took the subway.           4   \n",
       "\n",
       "        NE_count  %_NE_in_sentence  target_text_length  \n",
       "0              5               0.3                 108  \n",
       "1              1               0.0                 144  \n",
       "2              0               0.0                 332  \n",
       "3              1               0.0                 116  \n",
       "4              0               0.0                  10  \n",
       "...          ...               ...                 ...  \n",
       "152498         4               0.4                  74  \n",
       "152499         0               0.0                  76  \n",
       "152500         2               0.2                  96  \n",
       "152501         2               0.2                  28  \n",
       "152502         0               0.0                  19  \n",
       "\n",
       "[152503 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_sample = training_data.sample(frac=0.4, random_state=1)\n",
    "\n",
    "training_data = pd.read_csv('./1-NER_Data/1-csv_format/train/training_data_full_sentence.csv')\n",
    "# training_data = pd.read_csv('./1-NER_Data/1-csv_format/train/lower_normal_training.csv')\n",
    "\n",
    "training_data = training_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "training_data['target_text_length'] = training_data['target_text'].apply(lambda x: len(x))\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff46795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[training_data['target_text_length']> 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9547bc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0983587208120496"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[training_data['target_text_length']> 512])*100/len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22e6af8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>target_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He made his Millwall debut on 2 May 1998 again...</td>\n",
       "      <td>He made his &lt;Millwall&gt; debut on 2 May 1998 aga...</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was revised to suit adult preferences and w...</td>\n",
       "      <td>It was revised to suit adult preferences and w...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The efforts to get to the people who are addic...</td>\n",
       "      <td>The efforts to get to the people who are addic...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the wild , their range has contracted due t...</td>\n",
       "      <td>In the wild , their range has contracted due t...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Second day</td>\n",
       "      <td>Second day</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152498</th>\n",
       "      <td>The annual Grand National horse race takes pla...</td>\n",
       "      <td>The annual &lt;Gran National&gt; horse race takes pl...</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152499</th>\n",
       "      <td>His servants said to him, `` We heard that the...</td>\n",
       "      <td>His servants said to him, `` We heard that the...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152500</th>\n",
       "      <td>Other disease-causing bacteria in this family ...</td>\n",
       "      <td>Other disease-causing bacteria in this family ...</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152501</th>\n",
       "      <td>Cercle Brugge 4 0 3 1 4 5 3</td>\n",
       "      <td>&lt;Cercl Brugge&gt; 4 0 3 1 4 5 3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152502</th>\n",
       "      <td>He took the subway.</td>\n",
       "      <td>He took the subway.</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152353 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_text  \\\n",
       "0       He made his Millwall debut on 2 May 1998 again...   \n",
       "1       It was revised to suit adult preferences and w...   \n",
       "2       The efforts to get to the people who are addic...   \n",
       "3       In the wild , their range has contracted due t...   \n",
       "4                                              Second day   \n",
       "...                                                   ...   \n",
       "152498  The annual Grand National horse race takes pla...   \n",
       "152499  His servants said to him, `` We heard that the...   \n",
       "152500  Other disease-causing bacteria in this family ...   \n",
       "152501                        Cercle Brugge 4 0 3 1 4 5 3   \n",
       "152502                                He took the subway.   \n",
       "\n",
       "                                              target_text  word_count  \\\n",
       "0       He made his <Millwall> debut on 2 May 1998 aga...          19   \n",
       "1       It was revised to suit adult preferences and w...          22   \n",
       "2       The efforts to get to the people who are addic...          60   \n",
       "3       In the wild , their range has contracted due t...          20   \n",
       "4                                              Second day           2   \n",
       "...                                                   ...         ...   \n",
       "152498  The annual <Gran National> horse race takes pl...          11   \n",
       "152499  His servants said to him, `` We heard that the...          15   \n",
       "152500  Other disease-causing bacteria in this family ...          10   \n",
       "152501                       <Cercl Brugge> 4 0 3 1 4 5 3           9   \n",
       "152502                                He took the subway.           4   \n",
       "\n",
       "        NE_count  %_NE_in_sentence  target_text_length  \n",
       "0              5               0.3                 108  \n",
       "1              1               0.0                 144  \n",
       "2              0               0.0                 332  \n",
       "3              1               0.0                 116  \n",
       "4              0               0.0                  10  \n",
       "...          ...               ...                 ...  \n",
       "152498         4               0.4                  74  \n",
       "152499         0               0.0                  76  \n",
       "152500         2               0.2                  96  \n",
       "152501         2               0.2                  28  \n",
       "152502         0               0.0                  19  \n",
       "\n",
       "[152353 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data.drop(training_data[training_data['target_text_length']> 512].index)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c582dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152353\n"
     ]
    }
   ],
   "source": [
    "X_train_tokenized = tokenizer(['ner: ' + sequence for sequence in training_data[\"input_text\"]], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_train_tokenized = tokenizer(list(training_data[\"target_text\"]), \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(training_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70de9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0085c065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>target_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is evidence of post holes from the woode...</td>\n",
       "      <td>There is evidence of post holes from the woode...</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bank of Montreal said it added 850 million Can...</td>\n",
       "      <td>&lt;Bank of Montreal&gt; said it added 850 million C...</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once again, we 're trying to get ahold of our ...</td>\n",
       "      <td>Once again, we 're trying to get ahold of our ...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In New South Wales , private ownership of an i...</td>\n",
       "      <td>In &lt;Ne Sout Wales&gt; , private ownership of an i...</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was a huge fear of mine that it would n't w...</td>\n",
       "      <td>It was a huge fear of mine that it would n't w...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24845</th>\n",
       "      <td>Previously , WJZ-TV carried the team from thei...</td>\n",
       "      <td>Previously , &lt;WJZ-TV&gt; carried the team from th...</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24846</th>\n",
       "      <td>Robert Stovall, a veteran New York money manag...</td>\n",
       "      <td>&lt;Robert Stovall&gt;, a veteran &lt;New York&gt; money m...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24847</th>\n",
       "      <td>Hindenburg refused the powers but agreed to th...</td>\n",
       "      <td>&lt;Hindenburg&gt; refused the powers but agreed to ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24848</th>\n",
       "      <td>He finished fourth in the Olympics that year .</td>\n",
       "      <td>He finished fourth in the &lt;Olympics&gt; that year .</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24849</th>\n",
       "      <td>CHICAGO 65 66 .496 5</td>\n",
       "      <td>&lt;CHICAGO&gt; 65 66 .496 5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24850 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_text  \\\n",
       "0      There is evidence of post holes from the woode...   \n",
       "1      Bank of Montreal said it added 850 million Can...   \n",
       "2      Once again, we 're trying to get ahold of our ...   \n",
       "3      In New South Wales , private ownership of an i...   \n",
       "4      It was a huge fear of mine that it would n't w...   \n",
       "...                                                  ...   \n",
       "24845  Previously , WJZ-TV carried the team from thei...   \n",
       "24846  Robert Stovall, a veteran New York money manag...   \n",
       "24847  Hindenburg refused the powers but agreed to th...   \n",
       "24848     He finished fourth in the Olympics that year .   \n",
       "24849                               CHICAGO 65 66 .496 5   \n",
       "\n",
       "                                             target_text  word_count  \\\n",
       "0      There is evidence of post holes from the woode...          34   \n",
       "1      <Bank of Montreal> said it added 850 million C...          33   \n",
       "2      Once again, we 're trying to get ahold of our ...          20   \n",
       "3      In <Ne Sout Wales> , private ownership of an i...          20   \n",
       "4      It was a huge fear of mine that it would n't w...          26   \n",
       "...                                                  ...         ...   \n",
       "24845  Previously , <WJZ-TV> carried the team from th...          14   \n",
       "24846  <Robert Stovall>, a veteran <New York> money m...          21   \n",
       "24847  <Hindenburg> refused the powers but agreed to ...           9   \n",
       "24848   He finished fourth in the <Olympics> that year .           8   \n",
       "24849                             <CHICAGO> 65 66 .496 5           5   \n",
       "\n",
       "       NE_count  %_NE_in_sentence  target_text_length  \n",
       "0             2               0.1                 196  \n",
       "1             1               0.0                 203  \n",
       "2             0               0.0                  92  \n",
       "3             3               0.2                 122  \n",
       "4             0               0.0                 106  \n",
       "...         ...               ...                 ...  \n",
       "24845         2               0.1                  95  \n",
       "24846         3               0.1                 147  \n",
       "24847         1               0.1                  60  \n",
       "24848         1               0.1                  48  \n",
       "24849         1               0.2                  22  \n",
       "\n",
       "[24850 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = pd.read_csv('./1-NER_Data/1-csv_format/val/val_data_full_sentence.csv')\n",
    "validation_data = validation_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "validation_data['target_text_length'] = validation_data['target_text'].apply(lambda x: len(x))\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aab4049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>NE_count</th>\n",
       "      <th>%_NE_in_sentence</th>\n",
       "      <th>target_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is evidence of post holes from the woode...</td>\n",
       "      <td>There is evidence of post holes from the woode...</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bank of Montreal said it added 850 million Can...</td>\n",
       "      <td>&lt;Bank of Montreal&gt; said it added 850 million C...</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once again, we 're trying to get ahold of our ...</td>\n",
       "      <td>Once again, we 're trying to get ahold of our ...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In New South Wales , private ownership of an i...</td>\n",
       "      <td>In &lt;Ne Sout Wales&gt; , private ownership of an i...</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was a huge fear of mine that it would n't w...</td>\n",
       "      <td>It was a huge fear of mine that it would n't w...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24845</th>\n",
       "      <td>Previously , WJZ-TV carried the team from thei...</td>\n",
       "      <td>Previously , &lt;WJZ-TV&gt; carried the team from th...</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24846</th>\n",
       "      <td>Robert Stovall, a veteran New York money manag...</td>\n",
       "      <td>&lt;Robert Stovall&gt;, a veteran &lt;New York&gt; money m...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24847</th>\n",
       "      <td>Hindenburg refused the powers but agreed to th...</td>\n",
       "      <td>&lt;Hindenburg&gt; refused the powers but agreed to ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24848</th>\n",
       "      <td>He finished fourth in the Olympics that year .</td>\n",
       "      <td>He finished fourth in the &lt;Olympics&gt; that year .</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24849</th>\n",
       "      <td>CHICAGO 65 66 .496 5</td>\n",
       "      <td>&lt;CHICAGO&gt; 65 66 .496 5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24832 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_text  \\\n",
       "0      There is evidence of post holes from the woode...   \n",
       "1      Bank of Montreal said it added 850 million Can...   \n",
       "2      Once again, we 're trying to get ahold of our ...   \n",
       "3      In New South Wales , private ownership of an i...   \n",
       "4      It was a huge fear of mine that it would n't w...   \n",
       "...                                                  ...   \n",
       "24845  Previously , WJZ-TV carried the team from thei...   \n",
       "24846  Robert Stovall, a veteran New York money manag...   \n",
       "24847  Hindenburg refused the powers but agreed to th...   \n",
       "24848     He finished fourth in the Olympics that year .   \n",
       "24849                               CHICAGO 65 66 .496 5   \n",
       "\n",
       "                                             target_text  word_count  \\\n",
       "0      There is evidence of post holes from the woode...          34   \n",
       "1      <Bank of Montreal> said it added 850 million C...          33   \n",
       "2      Once again, we 're trying to get ahold of our ...          20   \n",
       "3      In <Ne Sout Wales> , private ownership of an i...          20   \n",
       "4      It was a huge fear of mine that it would n't w...          26   \n",
       "...                                                  ...         ...   \n",
       "24845  Previously , <WJZ-TV> carried the team from th...          14   \n",
       "24846  <Robert Stovall>, a veteran <New York> money m...          21   \n",
       "24847  <Hindenburg> refused the powers but agreed to ...           9   \n",
       "24848   He finished fourth in the <Olympics> that year .           8   \n",
       "24849                             <CHICAGO> 65 66 .496 5           5   \n",
       "\n",
       "       NE_count  %_NE_in_sentence  target_text_length  \n",
       "0             2               0.1                 196  \n",
       "1             1               0.0                 203  \n",
       "2             0               0.0                  92  \n",
       "3             3               0.2                 122  \n",
       "4             0               0.0                 106  \n",
       "...         ...               ...                 ...  \n",
       "24845         2               0.1                  95  \n",
       "24846         3               0.1                 147  \n",
       "24847         1               0.1                  60  \n",
       "24848         1               0.1                  48  \n",
       "24849         1               0.2                  22  \n",
       "\n",
       "[24832 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = validation_data.drop(validation_data[validation_data['target_text_length']> 512].index)\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6024ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24832\n"
     ]
    }
   ],
   "source": [
    "X_val_tokenized = tokenizer(['ner: ' + sequence for sequence in validation_data[\"input_text\"]], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_val_tokenized = tokenizer(list(validation_data[\"target_text\"]), \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(validation_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bafff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e605e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(X_val_tokenized, y_val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b54fe0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     \"NER_lower\",\n",
    "#     evaluation_strategy ='steps',\n",
    "#     eval_steps = 500, # Evaluation and Save happens every 500 steps\n",
    "#     save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "#     per_device_train_batch_size = 8,\n",
    "#     per_device_eval_batch_size = 8,\n",
    "#     learning_rate = 1e-3,\n",
    "#     adam_epsilon = 1e-8,\n",
    "#     num_train_epochs = 6,\n",
    "#     report_to=\"wandb\",\n",
    "# #     metric_for_best_model = 'f1',\n",
    "#     load_best_model_at_end=True\n",
    "# )\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"NER_full_sent\",\n",
    "#     evaluation_strategy ='epoch',\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 2380, # Evaluation and Save happens every 3743 steps (steps/(num of epochs*2))\n",
    "#     logging_steps = 500,\n",
    "    save_steps = 2380,\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    \n",
    "    gradient_accumulation_steps =2,\n",
    "    \n",
    "    learning_rate = 1e-3,\n",
    "    adam_epsilon = 1e-8,\n",
    "    num_train_epochs = 10,\n",
    "    report_to=\"wandb\",\n",
    "#     metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860a8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "#     compute_metrics=compute_metrics,\n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ecbfffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 152353\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 47610\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/HadyElkady/work/Bachelor_thesis/wandb/run-20220823_211609-1oouqan6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hodz199/NER_full_sentence/runs/1oouqan6\" target=\"_blank\">NER_full_sent</a></strong> to <a href=\"https://wandb.ai/hodz199/NER_full_sentence\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='47610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/47610 : < :, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.91 GiB total capacity; 4.48 GiB already allocated; 6.00 MiB free; 4.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 if (\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                \"of them.\")\n\u001b[1;32m    198\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mReduceAddCoalesced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, destination, num_inputs, *grads)\u001b[0m\n\u001b[1;32m     43\u001b[0m         grads_ = [grads[i:i + num_inputs]\n\u001b[1;32m     44\u001b[0m                   for i in range(0, len(grads), num_inputs)]\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_add_coalesced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/comm.py\u001b[0m in \u001b[0;36mreduce_add_coalesced\u001b[0;34m(inputs, destination, buffer_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# now the dense ones, which have consistent sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_flatten_dense_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (num_gpus,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mflat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_unflatten_dense_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/comm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# now the dense ones, which have consistent sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_flatten_dense_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (num_gpus,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mflat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_unflatten_dense_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_flatten_dense_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mcontiguous\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0mD\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0minput\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_dense_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.91 GiB total capacity; 4.48 GiB already allocated; 6.00 MiB free; 4.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
