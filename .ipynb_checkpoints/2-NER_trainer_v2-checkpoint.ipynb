{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2910e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhodz199\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=NER_v2\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1' \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,3\" \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT= NER_v2\n",
    "\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a13f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "# https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "569d8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0464c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration \n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_sample = training_data.sample(frac=0.4, random_state=1)\n",
    "\n",
    "training_data = pd.read_csv('./1-NER_Data/1-csv_format/train/training_data.csv')\n",
    "# training_data = pd.read_csv('./1-NER_Data/1-csv_format/train/lower_normal_training.csv')\n",
    "\n",
    "training_data = training_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "training_data['input_length'] = training_data['input_text'].apply(lambda x: len(x))\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.drop(training_data[training_data['input_length']> 512].index)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer(['ner: ' + sequence for sequence in training_data[\"input_text\"]], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_train_tokenized = tokenizer(list(training_data[\"target_text\"]), \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(training_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c384b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('./1-NER_Data/1-csv_format/val/val_data.csv')\n",
    "validation_data = validation_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "validation_data['input_length'] = validation_data['input_text'].apply(lambda x: len(x))\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = validation_data.drop(validation_data[validation_data['input_length']> 512].index)\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ce22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tokenized = tokenizer(['ner: ' + sequence for sequence in validation_data[\"input_text\"]], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_val_tokenized = tokenizer(list(validation_data[\"target_text\"]), \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(validation_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(X_val_tokenized, y_val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8be1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     \"NER_lower\",\n",
    "#     evaluation_strategy ='steps',\n",
    "#     eval_steps = 500, # Evaluation and Save happens every 500 steps\n",
    "#     save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "#     per_device_train_batch_size = 8,\n",
    "#     per_device_eval_batch_size = 8,\n",
    "#     learning_rate = 1e-3,\n",
    "#     adam_epsilon = 1e-8,\n",
    "#     num_train_epochs = 6,\n",
    "#     report_to=\"wandb\",\n",
    "# #     metric_for_best_model = 'f1',\n",
    "#     load_best_model_at_end=True\n",
    "# )\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"NER_normal\",\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 5000, # Evaluation and Save happens every 500 steps\n",
    "    save_steps = 5000,\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    per_device_train_batch_size = 7,\n",
    "    per_device_eval_batch_size = 7,\n",
    "    learning_rate = 1e-3,\n",
    "    adam_epsilon = 1e-8,\n",
    "    num_train_epochs = 12,\n",
    "    report_to=\"wandb\",\n",
    "#     metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9746036",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fccbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6201a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num examples = 317864\n",
    "# Num Epochs = 3\n",
    "# Instantaneous batch size per device = 8\n",
    "# Total train batch size (w. parallel, distributed & accumulation) = 32\n",
    "# Gradient Accumulation steps = 1\n",
    "# Total optimization steps = 29802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ner normal ###\n",
    "# number of steps -> 29802\n",
    "# epochs -> 6\n",
    "# patience -> 5\n",
    "# eval_steps = 1000\n",
    "# save_steps = 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
