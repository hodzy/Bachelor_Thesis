{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67395e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\" \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:2\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bf7c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "# https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e088823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 1024\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b0d9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration \n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9512cde7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>entity</th>\n",
       "      <th>wikidata_reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19399</th>\n",
       "      <td>Q823600</td>\n",
       "      <td>**Who is someone that was born in chesterfield?**</td>\n",
       "      <td>**chesterfield**</td>\n",
       "      <td>**[[Q2414206, Chesterfield, town in Massachuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14726</th>\n",
       "      <td>Q3660532</td>\n",
       "      <td>**what gender is carole hayman**</td>\n",
       "      <td>**carole hayman**</td>\n",
       "      <td>**[[Q3660532, Carole Hayman, British writer]]**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20385</th>\n",
       "      <td>Q1290008</td>\n",
       "      <td>**Which country is marcel landers from?**</td>\n",
       "      <td>**marcel landers**</td>\n",
       "      <td>**[[Q1290008, Marcel Landers, German footballe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20463</th>\n",
       "      <td>Q238440</td>\n",
       "      <td>**where did the battle of navarino take place**</td>\n",
       "      <td>**battle of navarino**</td>\n",
       "      <td>**[[Q238440, Battle of Navarino, 1827 naval ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5886</th>\n",
       "      <td>Q65598</td>\n",
       "      <td>**what nationality is jürgen röber**</td>\n",
       "      <td>**jürgen röber**</td>\n",
       "      <td>**[[Q65598, Jürgen Röber, football player and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813</th>\n",
       "      <td>Q3534673</td>\n",
       "      <td>**what is m. k. muthu known for being**</td>\n",
       "      <td>**m. k. muthu**</td>\n",
       "      <td>**[[Q6712758, M. K. Muthukaruppannasamy, Membe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32511</th>\n",
       "      <td>Q381178</td>\n",
       "      <td>**where did bob barker grow up?**</td>\n",
       "      <td>**bob barker**</td>\n",
       "      <td>**[[Q55762050, Bob Barker Company, American co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>Q1143438</td>\n",
       "      <td>**is the mysterious mr quin fiction or nonfict...</td>\n",
       "      <td>**the mysterious mr quin**</td>\n",
       "      <td>**[[Q1143438, The Mysterious Mr Quin, book]]**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>Q3086842</td>\n",
       "      <td>**Which film is fred guiol a director for**</td>\n",
       "      <td>**fred guiol**</td>\n",
       "      <td>**[[Q3086842, Fred Guiol, Film director; scree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003</th>\n",
       "      <td>Q16149111</td>\n",
       "      <td>**who is the new governor of oregon?**</td>\n",
       "      <td>**governor of oregon**</td>\n",
       "      <td>**[[Q16149111, Governor of Oregon, head of sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             qid                                           question  \\\n",
       "19399    Q823600  **Who is someone that was born in chesterfield?**   \n",
       "14726   Q3660532                   **what gender is carole hayman**   \n",
       "20385   Q1290008          **Which country is marcel landers from?**   \n",
       "20463    Q238440    **where did the battle of navarino take place**   \n",
       "5886      Q65598               **what nationality is jürgen röber**   \n",
       "...          ...                                                ...   \n",
       "7813    Q3534673            **what is m. k. muthu known for being**   \n",
       "32511    Q381178                  **where did bob barker grow up?**   \n",
       "5192    Q1143438  **is the mysterious mr quin fiction or nonfict...   \n",
       "12172   Q3086842        **Which film is fred guiol a director for**   \n",
       "33003  Q16149111             **who is the new governor of oregon?**   \n",
       "\n",
       "                           entity  \\\n",
       "19399            **chesterfield**   \n",
       "14726           **carole hayman**   \n",
       "20385          **marcel landers**   \n",
       "20463      **battle of navarino**   \n",
       "5886             **jürgen röber**   \n",
       "...                           ...   \n",
       "7813              **m. k. muthu**   \n",
       "32511              **bob barker**   \n",
       "5192   **the mysterious mr quin**   \n",
       "12172              **fred guiol**   \n",
       "33003      **governor of oregon**   \n",
       "\n",
       "                                          wikidata_reply  \n",
       "19399  **[[Q2414206, Chesterfield, town in Massachuse...  \n",
       "14726    **[[Q3660532, Carole Hayman, British writer]]**  \n",
       "20385  **[[Q1290008, Marcel Landers, German footballe...  \n",
       "20463  **[[Q238440, Battle of Navarino, 1827 naval ba...  \n",
       "5886   **[[Q65598, Jürgen Röber, football player and ...  \n",
       "...                                                  ...  \n",
       "7813   **[[Q6712758, M. K. Muthukaruppannasamy, Membe...  \n",
       "32511  **[[Q55762050, Bob Barker Company, American co...  \n",
       "5192      **[[Q1143438, The Mysterious Mr Quin, book]]**  \n",
       "12172  **[[Q3086842, Fred Guiol, Film director; scree...  \n",
       "33003  **[[Q16149111, Governor of Oregon, head of sta...  \n",
       "\n",
       "[34300 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv('./2-NEL_Data/2-csv_format_2/training_data_shuffled.csv')\n",
    "training_data = training_data.sample(frac=1, random_state=1)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38a6dd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Who is someone that was born in chesterfield?**,**chesterfield**,**[[Q2414206, Chesterfield, town in Massachusetts, USA], [Q1924516, Chesterfield, human settlement in Indiana, United States of America], [Q823600, Chesterfield, market town and unparished area in Derbyshire, England], [Q959443, Chesterfield, city in St. Louis County, Missouri, United States], [Q2310682, Chesterfield, town in New Hampshire], [Q2063962, Chesterfield, town in Chesterfield County, South Carolina, United States], [Q48935, Chesterfield F.C., association football club in Chesterfield, England]]**'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = list(training_data['question'] + ',' + training_data['entity'] + ',' + training_data['wikidata_reply'])\n",
    "input_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07436933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q823600'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text = list(training_data['qid'])\n",
    "target_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edace39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34300\n"
     ]
    }
   ],
   "source": [
    "X_train_tokenized = tokenizer(['nel: ' + sequence for sequence in input_text], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_train_tokenized = tokenizer(target_text, \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(training_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2ceaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981987dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c990424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "      <th>entity</th>\n",
       "      <th>wikidata_reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>Q149941</td>\n",
       "      <td>**what country is alaa abd el-fattah from**</td>\n",
       "      <td>**alaa abd el-fattah**</td>\n",
       "      <td>**[[Q149941, Alaa Abd El-Fattah, Egyptian huma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>Q11425</td>\n",
       "      <td>**What's an example of an animation program?**</td>\n",
       "      <td>**animation**</td>\n",
       "      <td>**[[Q11425, animation, method of creating movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>Q131433</td>\n",
       "      <td>**who is shania twain's husband?**</td>\n",
       "      <td>**shania twain**</td>\n",
       "      <td>**[[Q1143593, Shania Twain discography, Wikime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>Q1324387</td>\n",
       "      <td>**where was el medico born**</td>\n",
       "      <td>**el medico**</td>\n",
       "      <td>**[[Q1324387, El Médico, Cuban musician], [Q27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>Q3282637</td>\n",
       "      <td>**who is film producer**</td>\n",
       "      <td>**film producer**</td>\n",
       "      <td>**[[Q111316788, Chandni Soni, Film Producer], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>Q1868197</td>\n",
       "      <td>**Where was wisdom agblexo born**</td>\n",
       "      <td>**wisdom agblexo**</td>\n",
       "      <td>**[[Q1868197, Wisdom Agblexo, Ghanaian footbal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>Q93196</td>\n",
       "      <td>**what is the name of a bollywood movice**</td>\n",
       "      <td>**bollywood**</td>\n",
       "      <td>**[[Q110592757, Music Videos &gt; Indian &gt; Bollyw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Q691225</td>\n",
       "      <td>**What does a lineman (occupation) specialize ...</td>\n",
       "      <td>**lineman**</td>\n",
       "      <td>**[[Q269359, \"lineman's pliers\", multifunction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>Q15998685</td>\n",
       "      <td>**Where was william j. heffernan bown**</td>\n",
       "      <td>**william j. heffernan**</td>\n",
       "      <td>**[[Q15998685, William J. Heffernan, American ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Q6282391</td>\n",
       "      <td>**Which city in America did joseph curran die ...</td>\n",
       "      <td>**joseph curran**</td>\n",
       "      <td>**[[Q6282395, Joseph Curran Morrison, Canadian...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            qid                                           question  \\\n",
       "910     Q149941        **what country is alaa abd el-fattah from**   \n",
       "1715     Q11425     **What's an example of an animation program?**   \n",
       "4700    Q131433                 **who is shania twain's husband?**   \n",
       "2776   Q1324387                       **where was el medico born**   \n",
       "4284   Q3282637                           **who is film producer**   \n",
       "...         ...                                                ...   \n",
       "2895   Q1868197                  **Where was wisdom agblexo born**   \n",
       "2763     Q93196         **what is the name of a bollywood movice**   \n",
       "905     Q691225  **What does a lineman (occupation) specialize ...   \n",
       "3980  Q15998685            **Where was william j. heffernan bown**   \n",
       "235    Q6282391  **Which city in America did joseph curran die ...   \n",
       "\n",
       "                        entity  \\\n",
       "910     **alaa abd el-fattah**   \n",
       "1715             **animation**   \n",
       "4700          **shania twain**   \n",
       "2776             **el medico**   \n",
       "4284         **film producer**   \n",
       "...                        ...   \n",
       "2895        **wisdom agblexo**   \n",
       "2763             **bollywood**   \n",
       "905                **lineman**   \n",
       "3980  **william j. heffernan**   \n",
       "235          **joseph curran**   \n",
       "\n",
       "                                         wikidata_reply  \n",
       "910   **[[Q149941, Alaa Abd El-Fattah, Egyptian huma...  \n",
       "1715  **[[Q11425, animation, method of creating movi...  \n",
       "4700  **[[Q1143593, Shania Twain discography, Wikime...  \n",
       "2776  **[[Q1324387, El Médico, Cuban musician], [Q27...  \n",
       "4284  **[[Q111316788, Chandni Soni, Film Producer], ...  \n",
       "...                                                 ...  \n",
       "2895  **[[Q1868197, Wisdom Agblexo, Ghanaian footbal...  \n",
       "2763  **[[Q110592757, Music Videos > Indian > Bollyw...  \n",
       "905   **[[Q269359, \"lineman's pliers\", multifunction...  \n",
       "3980  **[[Q15998685, William J. Heffernan, American ...  \n",
       "235   **[[Q6282395, Joseph Curran Morrison, Canadian...  \n",
       "\n",
       "[4846 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = pd.read_csv('./2-NEL_Data/2-csv_format_2/val_data_shuffled.csv')\n",
    "val_data = val_data.sample(frac=1, random_state=1)\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5659295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**what country is alaa abd el-fattah from**,**alaa abd el-fattah**,**[[Q149941, Alaa Abd El-Fattah, Egyptian human rights activist]]**'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_val = list(val_data['question'] + ',' + val_data['entity'] + ',' + val_data['wikidata_reply'])\n",
    "input_text_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ae729d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q149941'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_val = list(val_data['qid'])\n",
    "target_text_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c443e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4846\n"
     ]
    }
   ],
   "source": [
    "X_val_tokenized = tokenizer(['nel: ' + sequence for sequence in input_text_val], \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_source_length)\n",
    "\n",
    "y_val_tokenized = tokenizer(target_text_val, \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=max_target_length)\n",
    "\n",
    "print(len(val_data))\n",
    "# print(len(training_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e4316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e30ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dcf5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4576754",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(X_val_tokenized, y_val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35d2948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"NEL_model_normal_shuffled\",\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 500, # Evaluation and Save happens every 50 steps\n",
    "    logging_steps = 500,\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    learning_rate = 1e-3,\n",
    "    adam_epsilon = 1e-8,\n",
    "    num_train_epochs = 5,\n",
    "    report_to=\"wandb\",\n",
    "#     metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3b330a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ca6f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 34300\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14295\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/HadyElkady/work/Bachelor_thesis/wandb/run-20220720_125215-2j701fge</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hodz199/huggingface/runs/2j701fge\" target=\"_blank\">NEL_model_normal_shuffled</a></strong> to <a href=\"https://wandb.ai/hodz199/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8500' max='14295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8500/14295 1:40:57 < 1:08:51, 1.40 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.694200</td>\n",
       "      <td>0.463458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.355211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.284501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.336900</td>\n",
       "      <td>0.278281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.267376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.234602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.245300</td>\n",
       "      <td>0.250234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>0.242878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.216075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.225882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.214713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.203559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.222718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.186075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.197628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.192523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.192075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-500\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-500/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-500/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-1000\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-1000/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-1000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-1500\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-1500/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-1500/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-2000\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-2000/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-2000/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-2500\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-2500/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-2500/pytorch_model.bin\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-3000\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-3000/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-3500\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-3500/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-1000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-4000\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-4000/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-1500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-4500\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-4500/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-2000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-5000\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-5000/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-2500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-5500\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-5500/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-3000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-6000\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-6000/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-3500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-6500\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-6500/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-4000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-7000\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-7000/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-4500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-7500\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-7500/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-5000] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-8000\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-8000/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-5500] due to args.save_total_limit\n",
      "/home/HadyElkady/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4846\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to NEL_model_normal_shuffled/checkpoint-8500\n",
      "Configuration saved in NEL_model_normal_shuffled/checkpoint-8500/config.json\n",
      "Model weights saved in NEL_model_normal_shuffled/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [NEL_model_normal_shuffled/checkpoint-6000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from NEL_model_normal_shuffled/checkpoint-7000 (score: 0.18607528507709503).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8500, training_loss=0.27528705731560205, metrics={'train_runtime': 6064.0706, 'train_samples_per_second': 28.281, 'train_steps_per_second': 2.357, 'total_flos': 4.742704873193472e+16, 'train_loss': 0.27528705731560205, 'epoch': 2.97})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1b764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
